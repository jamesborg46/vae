{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/indoril007/vae-project\" target=\"_blank\">https://app.wandb.ai/indoril007/vae-project</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/indoril007/vae-project/runs/qa2355py\" target=\"_blank\">https://app.wandb.ai/indoril007/vae-project/runs/qa2355py</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn import init\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"vae-project\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, units=500, z_dim=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(28*28, units)\n",
    "        self.mean = nn.Linear(units, z_dim)\n",
    "        self.log_var = nn.Linear(units, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.dense(x))\n",
    "        mean = self.mean(x)\n",
    "        log_var = self.log_var(x)\n",
    "        var = torch.exp(log_var)\n",
    "        std = torch.sqrt(var)\n",
    "        \n",
    "        self.kl_loss = -(0.5 * torch.sum(1 + log_var - mean**2 - var))\n",
    "        return mean, std\n",
    "\n",
    "class GaussianDecoder(nn.Module):\n",
    "    def __init__(self, z_dim=32, units=500):\n",
    "        super(GaussianDecoder, self).__init__()\n",
    "        self.dense = nn.Linear(z_dim, units)\n",
    "        self.mean = nn.Linear(units, 28*28)\n",
    "        self.log_var = nn.Linear(units, 28*28)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = torch.tanh(self.dense(z))\n",
    "        mean = self.mean(z)\n",
    "        log_var = self.log_var(z)\n",
    "        var = torch.exp(log_var)\n",
    "        std = torch.sqrt(var)\n",
    "        return mean, std\n",
    "\n",
    "class BenroulliDecoder(nn.Module):\n",
    "    def __init__(self, z_dim=32, units=500):\n",
    "        super(BenroulliDecoder, self).__init__()\n",
    "        self.dense = nn.Linear(z_dim, units)\n",
    "        self.out = nn.Linear(units, 28*28)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = torch.tanh(self.dense(z))\n",
    "        out = torch.sigmoid(self.out(z))\n",
    "        return out\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim=latent_dim)\n",
    "        self.decoder = GaussianDecoder(z_dim=latent_dim)\n",
    "        \n",
    "    def forward(self, x, num_samples=1):\n",
    "        batch_size = x.shape[0]\n",
    "        x = torch.flatten(x,1)\n",
    "        z_params = self.encoder(x)\n",
    "        z_samples = torch.distributions.Normal(*z_params).rsample((num_samples,))\n",
    "        x_params = self.decoder(z_samples)\n",
    "        reconstructed_x = torch.distributions.Normal(*x_params)\n",
    "        self.reconstruction_loss = -((1/num_samples) * torch.sum(reconstructed_x.log_prob(x)))\n",
    "        self.loss = self.encoder.kl_loss + self.reconstruction_loss\n",
    "        reconstructed_x = torch.reshape(reconstructed_x.sample(), (num_samples, batch_size, 1, 28, 28))\n",
    "        \n",
    "        return reconstructed_x\n",
    "\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, latent_dim=32):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.encoder = Encoder(z_dim=latent_dim)\n",
    "#         self.decoder = GaussianDecoder(z_dim=latent_dim)\n",
    "        \n",
    "#     def forward(self, x, num_samples=1):\n",
    "#         batch_size = x.shape[0]\n",
    "#         x = torch.flatten(x,1)\n",
    "#         z = self.encoder(x)\n",
    "#         samples = z.rsample((num_samples, ))\n",
    "#         reconstructed_x = self.decoder(samples)\n",
    "#         self.reconstruction_loss = -((1/num_samples) * torch.sum(F.))\n",
    "#         self.loss = self.encoder.kl_loss + self.reconstruction_loss\n",
    "#         reconstructed_x = torch.reshape(reconstructed_x.rsample(), (num_samples, batch_size, 28, 28))\n",
    "#         return reconstructed_x\n",
    "    \n",
    "    \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = (1/128) * model.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            wandb.log({\"training loss\": model.loss, \n",
    "                       \"training kl loss\": model.encoder.kl_loss / 128, \n",
    "                       \"training reconstruction loss\": model.reconstruction_loss / 128})\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    kl_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += ((1/10000) * model.loss).item()  # sum up batch loss\n",
    "            kl_loss += ((1/10000) * model.encoder.kl_loss).item()\n",
    "            reconstruction_loss += ((1/10000) * model.reconstruction_loss).item()\n",
    "        \n",
    "    wandb.log({\"test loss\": test_loss, \n",
    "           \"test kl loss\": kl_loss, \n",
    "           \"test reconstruction loss\": reconstruction_loss})\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        z = torch.distributions.Normal(loc=torch.zeros((5, 32,)), scale=torch.ones((5, 32,)))\n",
    "        z_params = model.decoder(z.sample().to(device))\n",
    "        samples = torch.reshape(torch.distributions.Normal(*z_params).sample(), (5, 28, 28)).cpu().numpy()\n",
    "        wandb.log({\"examples\" : [wandb.Image(i) for i in samples]})\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 1099.362671\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 141.965912\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 117.908699\n",
      "\n",
      "Test set: Average loss: 83.0499\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 103.185036\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 81.384750\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 66.534164\n",
      "\n",
      "Test set: Average loss: 42.4827\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 50.080872\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 227.022491\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 86.000450\n",
      "\n",
      "Test set: Average loss: 10.6581\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 19.416458\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 85.865837\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -70.420029\n",
      "\n",
      "Test set: Average loss: 6.8080\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: -39.233215\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 26.948807\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -41.806549\n",
      "\n",
      "Test set: Average loss: -25.5894\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: -88.165596\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -71.809761\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 82.264572\n",
      "\n",
      "Test set: Average loss: -56.0588\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: -105.338806\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 217.565491\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -128.520950\n",
      "\n",
      "Test set: Average loss: -60.0299\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: -65.111481\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -112.654602\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -41.749847\n",
      "\n",
      "Test set: Average loss: -109.5607\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: -134.479980\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -180.563965\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -154.629364\n",
      "\n",
      "Test set: Average loss: -129.3992\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: -140.594330\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -173.096405\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -109.245026\n",
      "\n",
      "Test set: Average loss: -146.3640\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: -189.898438\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 235.537750\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -149.165894\n",
      "\n",
      "Test set: Average loss: -168.2388\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: -207.439941\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: -164.557434\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: -110.651169\n",
      "\n",
      "Test set: Average loss: -174.1229\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: -149.328308\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: -199.151062\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: -166.689255\n",
      "\n",
      "Test set: Average loss: -182.0229\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: -248.513336\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 14.293854\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: -145.549927\n",
      "\n",
      "Test set: Average loss: -191.9383\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: -222.268250\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: -154.532288\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: -221.468338\n",
      "\n",
      "Test set: Average loss: -204.9828\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: -70.078384\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: -245.904175\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: -181.475967\n",
      "\n",
      "Test set: Average loss: -215.5820\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: -248.872498\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: -244.904175\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: -278.449860\n",
      "\n",
      "Test set: Average loss: -215.7276\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: -301.890747\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: -252.489410\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: -201.846832\n",
      "\n",
      "Test set: Average loss: -230.3409\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: -331.912415\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: -335.929382\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: -363.330017\n",
      "\n",
      "Test set: Average loss: -238.3949\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: -274.563354\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: -265.113739\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: -260.312500\n",
      "\n",
      "Test set: Average loss: -245.6697\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: -287.100037\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: -297.831696\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: -299.449402\n",
      "\n",
      "Test set: Average loss: -250.0247\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: -407.070618\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: -160.999817\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: -294.632141\n",
      "\n",
      "Test set: Average loss: -261.3177\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: -249.062866\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 33.092266\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: -287.471497\n",
      "\n",
      "Test set: Average loss: -274.1831\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: -300.156067\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: -381.542053\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: -98.313896\n",
      "\n",
      "Test set: Average loss: -273.4646\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: -322.646973\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: -250.302490\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: -313.607330\n",
      "\n",
      "Test set: Average loss: -278.2800\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: -288.684998\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: -297.502167\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: -298.019836\n",
      "\n",
      "Test set: Average loss: -273.7795\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: -318.424774\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: -322.663788\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: -406.837189\n",
      "\n",
      "Test set: Average loss: -283.9564\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: -374.311279\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: -111.811935\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: -237.926697\n",
      "\n",
      "Test set: Average loss: -298.9865\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: -113.416245\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: -145.985825\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: -398.847748\n",
      "\n",
      "Test set: Average loss: -299.7304\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: -401.100830\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: -365.661255\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: -399.319611\n",
      "\n",
      "Test set: Average loss: -312.5757\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: -399.942322\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: -306.104980\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: -268.605804\n",
      "\n",
      "Test set: Average loss: -322.7568\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: -330.899811\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: -389.203247\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: -307.822266\n",
      "\n",
      "Test set: Average loss: -324.3743\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: -330.797028\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: -322.758331\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: -174.937424\n",
      "\n",
      "Test set: Average loss: -329.3003\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: -244.002319\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: -343.375793\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: -185.657928\n",
      "\n",
      "Test set: Average loss: -321.0499\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: -426.977600\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: -253.636780\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: -455.823212\n",
      "\n",
      "Test set: Average loss: -337.1731\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: -388.560120\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: -312.490143\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: -385.299744\n",
      "\n",
      "Test set: Average loss: -343.2642\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: -389.344116\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: -350.069641\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: -244.112366\n",
      "\n",
      "Test set: Average loss: -350.2002\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: -365.231598\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: -345.940887\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: -287.662018\n",
      "\n",
      "Test set: Average loss: -333.8964\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: -453.475891\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: -197.207413\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: -300.457123\n",
      "\n",
      "Test set: Average loss: -349.3287\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: -319.396606\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: -37.680099\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: -369.939453\n",
      "\n",
      "Test set: Average loss: -352.8146\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: -375.266602\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: -303.351196\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: -393.284760\n",
      "\n",
      "Test set: Average loss: -362.1856\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: -338.007874\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: -395.450470\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: -388.627869\n",
      "\n",
      "Test set: Average loss: -377.7719\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: -347.325043\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: -350.861481\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: -306.882202\n",
      "\n",
      "Test set: Average loss: -377.2657\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: -329.039398\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: -342.651245\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: -322.835541\n",
      "\n",
      "Test set: Average loss: -359.5892\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: -392.960175\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: -327.989624\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: -343.157257\n",
      "\n",
      "Test set: Average loss: -383.9618\n",
      "\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: -487.435852\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: -446.213531\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: -466.674072\n",
      "\n",
      "Test set: Average loss: -385.4148\n",
      "\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: -37.637230\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: -345.138153\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: -327.118408\n",
      "\n",
      "Test set: Average loss: -392.3454\n",
      "\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: -419.947144\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: -387.649353\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: -427.272858\n",
      "\n",
      "Test set: Average loss: -386.4648\n",
      "\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: -406.381165\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: -246.272461\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: -432.811554\n",
      "\n",
      "Test set: Average loss: -331.7228\n",
      "\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: -315.845123\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: -332.374634\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: -430.972595\n",
      "\n",
      "Test set: Average loss: -375.8110\n",
      "\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: -442.431458\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: -420.947754\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: -446.268372\n",
      "\n",
      "Test set: Average loss: -409.9844\n",
      "\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: -497.334991\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: -548.700684\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: -397.278381\n",
      "\n",
      "Test set: Average loss: -406.3993\n",
      "\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: -405.364746\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: -457.608612\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: -413.925323\n",
      "\n",
      "Test set: Average loss: -413.3855\n",
      "\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: -372.676971\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: -381.973816\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: -613.500916\n",
      "\n",
      "Test set: Average loss: -415.0421\n",
      "\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: -474.501221\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: -366.344208\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: -508.652466\n",
      "\n",
      "Test set: Average loss: -292.3900\n",
      "\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: -401.593658\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: -425.914978\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: -395.432953\n",
      "\n",
      "Test set: Average loss: -428.2837\n",
      "\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: -469.014771\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: -471.890564\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: -375.553223\n",
      "\n",
      "Test set: Average loss: -431.4377\n",
      "\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: -425.715759\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: -536.071899\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: -413.520447\n",
      "\n",
      "Test set: Average loss: -438.1987\n",
      "\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: -471.290588\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: -344.403259\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: -424.719055\n",
      "\n",
      "Test set: Average loss: -443.2382\n",
      "\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: -428.496948\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: -537.307983\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: -490.486755\n",
      "\n",
      "Test set: Average loss: -452.6081\n",
      "\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: -474.926086\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: -529.833740\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: -448.165619\n",
      "\n",
      "Test set: Average loss: -433.7480\n",
      "\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: -331.953064\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: -448.704987\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: -493.591675\n",
      "\n",
      "Test set: Average loss: -449.5261\n",
      "\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: -490.017212\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: -501.124329\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: -554.180054\n",
      "\n",
      "Test set: Average loss: -446.3864\n",
      "\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: -441.682434\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: -377.929840\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: -484.667267\n",
      "\n",
      "Test set: Average loss: -460.6159\n",
      "\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: -553.608276\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: -463.101990\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: -519.487915\n",
      "\n",
      "Test set: Average loss: -472.9550\n",
      "\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: -567.724426\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tLoss: -622.911804\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: -235.821503\n",
      "\n",
      "Test set: Average loss: -464.5544\n",
      "\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: -527.067200\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tLoss: -516.214783\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: -463.908783\n",
      "\n",
      "Test set: Average loss: -460.2689\n",
      "\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: -496.145447\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tLoss: -285.480286\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: -645.863037\n",
      "\n",
      "Test set: Average loss: -467.4533\n",
      "\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: -579.156189\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tLoss: -417.634155\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: -469.647064\n",
      "\n",
      "Test set: Average loss: -431.3764\n",
      "\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: -490.125946\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tLoss: -459.028076\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: -480.434998\n",
      "\n",
      "Test set: Average loss: -477.0704\n",
      "\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: -632.122742\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tLoss: -459.906738\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: -412.438904\n",
      "\n",
      "Test set: Average loss: -475.0069\n",
      "\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: -521.685547\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tLoss: -562.911865\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: -394.713257\n",
      "\n",
      "Test set: Average loss: -479.4890\n",
      "\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: -494.536743\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tLoss: -495.717957\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: -484.214783\n",
      "\n",
      "Test set: Average loss: -490.7812\n",
      "\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: -379.655731\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tLoss: -509.595184\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: -511.309937\n",
      "\n",
      "Test set: Average loss: -481.3245\n",
      "\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: -540.035461\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tLoss: -594.589355\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: -459.449158\n",
      "\n",
      "Test set: Average loss: -481.2301\n",
      "\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: -579.954895\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tLoss: -475.949768\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: -552.765259\n",
      "\n",
      "Test set: Average loss: -448.8196\n",
      "\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: -473.098572\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tLoss: -579.234436\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: -569.558533\n",
      "\n",
      "Test set: Average loss: -479.8719\n",
      "\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: -611.377747\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tLoss: -517.926270\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tLoss: -640.729736\n",
      "\n",
      "Test set: Average loss: -509.4113\n",
      "\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: -624.359375\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tLoss: -578.283264\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tLoss: -448.017273\n",
      "\n",
      "Test set: Average loss: -484.7907\n",
      "\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: -593.141968\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tLoss: -454.493439\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tLoss: -566.331116\n",
      "\n",
      "Test set: Average loss: -502.1478\n",
      "\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: -588.101196\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tLoss: -491.441223\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tLoss: -481.154327\n",
      "\n",
      "Test set: Average loss: -509.1355\n",
      "\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: -552.010864\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tLoss: -517.550903\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tLoss: -490.424683\n",
      "\n",
      "Test set: Average loss: -515.5464\n",
      "\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: -468.349792\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tLoss: -452.561401\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tLoss: -587.174194\n",
      "\n",
      "Test set: Average loss: -514.9840\n",
      "\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: -615.834045\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tLoss: -575.086426\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tLoss: -488.821472\n",
      "\n",
      "Test set: Average loss: -518.4248\n",
      "\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: -483.140381\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tLoss: -523.866455\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tLoss: -121.537811\n",
      "\n",
      "Test set: Average loss: -507.6655\n",
      "\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: -513.773499\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tLoss: -557.430542\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tLoss: -537.689209\n",
      "\n",
      "Test set: Average loss: -506.7940\n",
      "\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: -476.273560\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tLoss: -537.296692\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tLoss: -164.199249\n",
      "\n",
      "Test set: Average loss: -519.1817\n",
      "\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: -573.114685\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tLoss: -398.786743\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tLoss: -121.995148\n",
      "\n",
      "Test set: Average loss: -526.1452\n",
      "\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: -590.260864\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tLoss: -641.844360\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tLoss: -599.040283\n",
      "\n",
      "Test set: Average loss: -527.3514\n",
      "\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: -537.012695\n",
      "Train Epoch: 89 [25600/60000 (43%)]\tLoss: -638.556335\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tLoss: -646.144470\n",
      "\n",
      "Test set: Average loss: -530.2729\n",
      "\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: -580.465515\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tLoss: -505.555969\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tLoss: -651.364075\n",
      "\n",
      "Test set: Average loss: -531.4893\n",
      "\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: -653.169739\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tLoss: -345.746979\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tLoss: -598.380859\n",
      "\n",
      "Test set: Average loss: -518.2696\n",
      "\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: -472.718140\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tLoss: -700.314087\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tLoss: -538.822876\n",
      "\n",
      "Test set: Average loss: -498.7566\n",
      "\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: -503.133179\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tLoss: -571.458069\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tLoss: -595.111450\n",
      "\n",
      "Test set: Average loss: -538.9551\n",
      "\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: -690.219360\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tLoss: -551.240356\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tLoss: -601.847290\n",
      "\n",
      "Test set: Average loss: -530.7421\n",
      "\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: -658.922974\n",
      "Train Epoch: 95 [25600/60000 (43%)]\tLoss: -582.997620\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tLoss: -581.772400\n",
      "\n",
      "Test set: Average loss: -546.3953\n",
      "\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: -579.874023\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tLoss: -603.117493\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tLoss: -595.268127\n",
      "\n",
      "Test set: Average loss: -562.6061\n",
      "\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: -700.267639\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tLoss: -508.538086\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tLoss: -495.655518\n",
      "\n",
      "Test set: Average loss: -554.1313\n",
      "\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: -581.528809\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tLoss: -601.304565\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tLoss: -440.480957\n",
      "\n",
      "Test set: Average loss: -545.7246\n",
      "\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: -650.792969\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tLoss: -594.122986\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tLoss: -457.145020\n",
      "\n",
      "Test set: Average loss: -553.9633\n",
      "\n",
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: -726.875000\n",
      "Train Epoch: 100 [25600/60000 (43%)]\tLoss: -657.077026\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tLoss: -649.439148\n",
      "\n",
      "Test set: Average loss: -550.6074\n",
      "\n",
      "Train Epoch: 101 [0/60000 (0%)]\tLoss: -683.279175\n",
      "Train Epoch: 101 [25600/60000 (43%)]\tLoss: -547.917114\n",
      "Train Epoch: 101 [51200/60000 (85%)]\tLoss: -613.848328\n",
      "\n",
      "Test set: Average loss: -541.8694\n",
      "\n",
      "Train Epoch: 102 [0/60000 (0%)]\tLoss: -581.319824\n",
      "Train Epoch: 102 [25600/60000 (43%)]\tLoss: -631.801514\n",
      "Train Epoch: 102 [51200/60000 (85%)]\tLoss: -540.581299\n",
      "\n",
      "Test set: Average loss: -549.1907\n",
      "\n",
      "Train Epoch: 103 [0/60000 (0%)]\tLoss: -423.962585\n",
      "Train Epoch: 103 [25600/60000 (43%)]\tLoss: -753.993958\n",
      "Train Epoch: 103 [51200/60000 (85%)]\tLoss: -609.521606\n",
      "\n",
      "Test set: Average loss: -559.8355\n",
      "\n",
      "Train Epoch: 104 [0/60000 (0%)]\tLoss: -602.545898\n",
      "Train Epoch: 104 [25600/60000 (43%)]\tLoss: -640.434143\n",
      "Train Epoch: 104 [51200/60000 (85%)]\tLoss: -641.684021\n",
      "\n",
      "Test set: Average loss: -561.5348\n",
      "\n",
      "Train Epoch: 105 [0/60000 (0%)]\tLoss: -690.461426\n",
      "Train Epoch: 105 [25600/60000 (43%)]\tLoss: -696.760193\n",
      "Train Epoch: 105 [51200/60000 (85%)]\tLoss: -683.578003\n",
      "\n",
      "Test set: Average loss: -551.1441\n",
      "\n",
      "Train Epoch: 106 [0/60000 (0%)]\tLoss: -302.254883\n",
      "Train Epoch: 106 [25600/60000 (43%)]\tLoss: -545.637878\n",
      "Train Epoch: 106 [51200/60000 (85%)]\tLoss: -542.531982\n",
      "\n",
      "Test set: Average loss: -532.4847\n",
      "\n",
      "Train Epoch: 107 [0/60000 (0%)]\tLoss: -533.510620\n",
      "Train Epoch: 107 [25600/60000 (43%)]\tLoss: -714.827637\n",
      "Train Epoch: 107 [51200/60000 (85%)]\tLoss: -504.413605\n",
      "\n",
      "Test set: Average loss: -577.2645\n",
      "\n",
      "Train Epoch: 108 [0/60000 (0%)]\tLoss: -622.098877\n",
      "Train Epoch: 108 [25600/60000 (43%)]\tLoss: -674.446960\n",
      "Train Epoch: 108 [51200/60000 (85%)]\tLoss: -637.963867\n",
      "\n",
      "Test set: Average loss: -511.2653\n",
      "\n",
      "Train Epoch: 109 [0/60000 (0%)]\tLoss: -660.346741\n",
      "Train Epoch: 109 [25600/60000 (43%)]\tLoss: -581.475220\n",
      "Train Epoch: 109 [51200/60000 (85%)]\tLoss: -633.141357\n",
      "\n",
      "Test set: Average loss: -576.8261\n",
      "\n",
      "Train Epoch: 110 [0/60000 (0%)]\tLoss: -683.929504\n",
      "Train Epoch: 110 [25600/60000 (43%)]\tLoss: -666.566772\n",
      "Train Epoch: 110 [51200/60000 (85%)]\tLoss: -564.475952\n",
      "\n",
      "Test set: Average loss: -555.0950\n",
      "\n",
      "Train Epoch: 111 [0/60000 (0%)]\tLoss: -670.761658\n",
      "Train Epoch: 111 [25600/60000 (43%)]\tLoss: -645.369751\n",
      "Train Epoch: 111 [51200/60000 (85%)]\tLoss: -711.220032\n",
      "\n",
      "Test set: Average loss: -572.6539\n",
      "\n",
      "Train Epoch: 112 [0/60000 (0%)]\tLoss: -652.884644\n",
      "Train Epoch: 112 [25600/60000 (43%)]\tLoss: -628.136719\n",
      "Train Epoch: 112 [51200/60000 (85%)]\tLoss: -572.590820\n",
      "\n",
      "Test set: Average loss: -577.4151\n",
      "\n",
      "Train Epoch: 113 [0/60000 (0%)]\tLoss: -694.203308\n",
      "Train Epoch: 113 [25600/60000 (43%)]\tLoss: -611.695923\n",
      "Train Epoch: 113 [51200/60000 (85%)]\tLoss: -635.122742\n",
      "\n",
      "Test set: Average loss: -574.5329\n",
      "\n",
      "Train Epoch: 114 [0/60000 (0%)]\tLoss: -656.615112\n",
      "Train Epoch: 114 [25600/60000 (43%)]\tLoss: -584.738403\n",
      "Train Epoch: 114 [51200/60000 (85%)]\tLoss: -394.228119\n",
      "\n",
      "Test set: Average loss: -572.8011\n",
      "\n",
      "Train Epoch: 115 [0/60000 (0%)]\tLoss: -657.219971\n",
      "Train Epoch: 115 [25600/60000 (43%)]\tLoss: -762.728821\n",
      "Train Epoch: 115 [51200/60000 (85%)]\tLoss: -692.160889\n",
      "\n",
      "Test set: Average loss: -573.0174\n",
      "\n",
      "Train Epoch: 116 [0/60000 (0%)]\tLoss: -651.835999\n",
      "Train Epoch: 116 [25600/60000 (43%)]\tLoss: -427.258453\n",
      "Train Epoch: 116 [51200/60000 (85%)]\tLoss: -685.354797\n",
      "\n",
      "Test set: Average loss: -581.0676\n",
      "\n",
      "Train Epoch: 117 [0/60000 (0%)]\tLoss: -564.581055\n",
      "Train Epoch: 117 [25600/60000 (43%)]\tLoss: -613.728577\n",
      "Train Epoch: 117 [51200/60000 (85%)]\tLoss: -671.136414\n",
      "\n",
      "Test set: Average loss: -569.0815\n",
      "\n",
      "Train Epoch: 118 [0/60000 (0%)]\tLoss: -590.834961\n",
      "Train Epoch: 118 [25600/60000 (43%)]\tLoss: -647.378418\n",
      "Train Epoch: 118 [51200/60000 (85%)]\tLoss: -673.301575\n",
      "\n",
      "Test set: Average loss: -514.9745\n",
      "\n",
      "Train Epoch: 119 [0/60000 (0%)]\tLoss: -631.971680\n",
      "Train Epoch: 119 [25600/60000 (43%)]\tLoss: -514.621094\n",
      "Train Epoch: 119 [51200/60000 (85%)]\tLoss: -519.484375\n",
      "\n",
      "Test set: Average loss: -587.7234\n",
      "\n",
      "Train Epoch: 120 [0/60000 (0%)]\tLoss: -621.015808\n",
      "Train Epoch: 120 [25600/60000 (43%)]\tLoss: -465.996155\n",
      "Train Epoch: 120 [51200/60000 (85%)]\tLoss: -558.360413\n",
      "\n",
      "Test set: Average loss: -573.3903\n",
      "\n",
      "Train Epoch: 121 [0/60000 (0%)]\tLoss: -687.867676\n",
      "Train Epoch: 121 [25600/60000 (43%)]\tLoss: -547.810059\n",
      "Train Epoch: 121 [51200/60000 (85%)]\tLoss: -745.505310\n",
      "\n",
      "Test set: Average loss: -590.8715\n",
      "\n",
      "Train Epoch: 122 [0/60000 (0%)]\tLoss: -621.812927\n",
      "Train Epoch: 122 [25600/60000 (43%)]\tLoss: -696.160522\n",
      "Train Epoch: 122 [51200/60000 (85%)]\tLoss: -477.158569\n",
      "\n",
      "Test set: Average loss: -561.3791\n",
      "\n",
      "Train Epoch: 123 [0/60000 (0%)]\tLoss: -640.849609\n",
      "Train Epoch: 123 [25600/60000 (43%)]\tLoss: -636.875977\n",
      "Train Epoch: 123 [51200/60000 (85%)]\tLoss: -427.735443\n",
      "\n",
      "Test set: Average loss: -577.5487\n",
      "\n",
      "Train Epoch: 124 [0/60000 (0%)]\tLoss: -694.650818\n",
      "Train Epoch: 124 [25600/60000 (43%)]\tLoss: -660.008606\n",
      "Train Epoch: 124 [51200/60000 (85%)]\tLoss: -416.636261\n",
      "\n",
      "Test set: Average loss: -573.1803\n",
      "\n",
      "Train Epoch: 125 [0/60000 (0%)]\tLoss: -739.757568\n",
      "Train Epoch: 125 [25600/60000 (43%)]\tLoss: -563.467102\n",
      "Train Epoch: 125 [51200/60000 (85%)]\tLoss: -619.719177\n",
      "\n",
      "Test set: Average loss: -501.0934\n",
      "\n",
      "Train Epoch: 126 [0/60000 (0%)]\tLoss: -648.693848\n",
      "Train Epoch: 126 [25600/60000 (43%)]\tLoss: -688.078125\n",
      "Train Epoch: 126 [51200/60000 (85%)]\tLoss: -663.890381\n",
      "\n",
      "Test set: Average loss: -588.7948\n",
      "\n",
      "Train Epoch: 127 [0/60000 (0%)]\tLoss: -601.265686\n",
      "Train Epoch: 127 [25600/60000 (43%)]\tLoss: -526.841431\n",
      "Train Epoch: 127 [51200/60000 (85%)]\tLoss: -678.523804\n",
      "\n",
      "Test set: Average loss: -605.0348\n",
      "\n",
      "Train Epoch: 128 [0/60000 (0%)]\tLoss: -610.871338\n",
      "Train Epoch: 128 [25600/60000 (43%)]\tLoss: -648.357300\n",
      "Train Epoch: 128 [51200/60000 (85%)]\tLoss: -742.652039\n",
      "\n",
      "Test set: Average loss: -558.0107\n",
      "\n",
      "Train Epoch: 129 [0/60000 (0%)]\tLoss: -703.502319\n",
      "Train Epoch: 129 [25600/60000 (43%)]\tLoss: -553.854492\n",
      "Train Epoch: 129 [51200/60000 (85%)]\tLoss: -583.033264\n",
      "\n",
      "Test set: Average loss: -591.3321\n",
      "\n",
      "Train Epoch: 130 [0/60000 (0%)]\tLoss: -611.162781\n",
      "Train Epoch: 130 [25600/60000 (43%)]\tLoss: -651.781433\n",
      "Train Epoch: 130 [51200/60000 (85%)]\tLoss: -662.642090\n",
      "\n",
      "Test set: Average loss: -565.9737\n",
      "\n",
      "Train Epoch: 131 [0/60000 (0%)]\tLoss: -646.558899\n",
      "Train Epoch: 131 [25600/60000 (43%)]\tLoss: -490.002594\n",
      "Train Epoch: 131 [51200/60000 (85%)]\tLoss: -547.396484\n",
      "\n",
      "Test set: Average loss: -565.0467\n",
      "\n",
      "Train Epoch: 132 [0/60000 (0%)]\tLoss: -627.373657\n",
      "Train Epoch: 132 [25600/60000 (43%)]\tLoss: -533.788879\n",
      "Train Epoch: 132 [51200/60000 (85%)]\tLoss: -648.271057\n",
      "\n",
      "Test set: Average loss: -596.3746\n",
      "\n",
      "Train Epoch: 133 [0/60000 (0%)]\tLoss: -670.455139\n",
      "Train Epoch: 133 [25600/60000 (43%)]\tLoss: -740.912781\n",
      "Train Epoch: 133 [51200/60000 (85%)]\tLoss: -468.244720\n",
      "\n",
      "Test set: Average loss: -582.4162\n",
      "\n",
      "Train Epoch: 134 [0/60000 (0%)]\tLoss: -459.125732\n",
      "Train Epoch: 134 [25600/60000 (43%)]\tLoss: -684.043884\n",
      "Train Epoch: 134 [51200/60000 (85%)]\tLoss: -619.908813\n",
      "\n",
      "Test set: Average loss: -566.6897\n",
      "\n",
      "Train Epoch: 135 [0/60000 (0%)]\tLoss: -681.355408\n",
      "Train Epoch: 135 [25600/60000 (43%)]\tLoss: -703.915894\n",
      "Train Epoch: 135 [51200/60000 (85%)]\tLoss: -536.364136\n",
      "\n",
      "Test set: Average loss: -574.0778\n",
      "\n",
      "Train Epoch: 136 [0/60000 (0%)]\tLoss: -656.525269\n",
      "Train Epoch: 136 [25600/60000 (43%)]\tLoss: -632.162231\n",
      "Train Epoch: 136 [51200/60000 (85%)]\tLoss: -760.049622\n",
      "\n",
      "Test set: Average loss: -600.4955\n",
      "\n",
      "Train Epoch: 137 [0/60000 (0%)]\tLoss: -676.154968\n",
      "Train Epoch: 137 [25600/60000 (43%)]\tLoss: -606.362793\n",
      "Train Epoch: 137 [51200/60000 (85%)]\tLoss: -585.107910\n",
      "\n",
      "Test set: Average loss: -587.4055\n",
      "\n",
      "Train Epoch: 138 [0/60000 (0%)]\tLoss: -640.180237\n",
      "Train Epoch: 138 [25600/60000 (43%)]\tLoss: -695.683167\n",
      "Train Epoch: 138 [51200/60000 (85%)]\tLoss: -806.911438\n",
      "\n",
      "Test set: Average loss: -583.4714\n",
      "\n",
      "Train Epoch: 139 [0/60000 (0%)]\tLoss: -717.127747\n",
      "Train Epoch: 139 [25600/60000 (43%)]\tLoss: -753.346008\n",
      "Train Epoch: 139 [51200/60000 (85%)]\tLoss: -727.052917\n",
      "\n",
      "Test set: Average loss: -591.9491\n",
      "\n",
      "Train Epoch: 140 [0/60000 (0%)]\tLoss: -484.089325\n",
      "Train Epoch: 140 [25600/60000 (43%)]\tLoss: -764.734802\n",
      "Train Epoch: 140 [51200/60000 (85%)]\tLoss: -714.097900\n",
      "\n",
      "Test set: Average loss: -599.0506\n",
      "\n",
      "Train Epoch: 141 [0/60000 (0%)]\tLoss: -772.030457\n",
      "Train Epoch: 141 [25600/60000 (43%)]\tLoss: -697.712097\n",
      "Train Epoch: 141 [51200/60000 (85%)]\tLoss: -719.834778\n",
      "\n",
      "Test set: Average loss: -590.0573\n",
      "\n",
      "Train Epoch: 142 [0/60000 (0%)]\tLoss: -821.675537\n",
      "Train Epoch: 142 [25600/60000 (43%)]\tLoss: -649.773071\n",
      "Train Epoch: 142 [51200/60000 (85%)]\tLoss: -533.384216\n",
      "\n",
      "Test set: Average loss: -610.9306\n",
      "\n",
      "Train Epoch: 143 [0/60000 (0%)]\tLoss: -827.235657\n",
      "Train Epoch: 143 [25600/60000 (43%)]\tLoss: -488.485535\n",
      "Train Epoch: 143 [51200/60000 (85%)]\tLoss: -708.608154\n",
      "\n",
      "Test set: Average loss: -596.2971\n",
      "\n",
      "Train Epoch: 144 [0/60000 (0%)]\tLoss: -800.213562\n",
      "Train Epoch: 144 [25600/60000 (43%)]\tLoss: -731.918945\n",
      "Train Epoch: 144 [51200/60000 (85%)]\tLoss: -675.860046\n",
      "\n",
      "Test set: Average loss: -602.1768\n",
      "\n",
      "Train Epoch: 145 [0/60000 (0%)]\tLoss: -636.063354\n",
      "Train Epoch: 145 [25600/60000 (43%)]\tLoss: -818.789917\n",
      "Train Epoch: 145 [51200/60000 (85%)]\tLoss: -294.276978\n",
      "\n",
      "Test set: Average loss: -561.8723\n",
      "\n",
      "Train Epoch: 146 [0/60000 (0%)]\tLoss: -578.689941\n",
      "Train Epoch: 146 [25600/60000 (43%)]\tLoss: -551.590881\n",
      "Train Epoch: 146 [51200/60000 (85%)]\tLoss: -633.312622\n",
      "\n",
      "Test set: Average loss: -612.9661\n",
      "\n",
      "Train Epoch: 147 [0/60000 (0%)]\tLoss: -688.798218\n",
      "Train Epoch: 147 [25600/60000 (43%)]\tLoss: -414.882751\n",
      "Train Epoch: 147 [51200/60000 (85%)]\tLoss: -647.239197\n",
      "\n",
      "Test set: Average loss: -582.0497\n",
      "\n",
      "Train Epoch: 148 [0/60000 (0%)]\tLoss: -687.774292\n",
      "Train Epoch: 148 [25600/60000 (43%)]\tLoss: -502.857605\n",
      "Train Epoch: 148 [51200/60000 (85%)]\tLoss: -693.988647\n",
      "\n",
      "Test set: Average loss: -616.1364\n",
      "\n",
      "Train Epoch: 149 [0/60000 (0%)]\tLoss: -755.969421\n",
      "Train Epoch: 149 [25600/60000 (43%)]\tLoss: -636.135071\n",
      "Train Epoch: 149 [51200/60000 (85%)]\tLoss: -732.803894\n",
      "\n",
      "Test set: Average loss: -585.3342\n",
      "\n",
      "Train Epoch: 150 [0/60000 (0%)]\tLoss: -649.129456\n",
      "Train Epoch: 150 [25600/60000 (43%)]\tLoss: -668.196045\n",
      "Train Epoch: 150 [51200/60000 (85%)]\tLoss: -707.511353\n",
      "\n",
      "Test set: Average loss: -602.2280\n",
      "\n",
      "Train Epoch: 151 [0/60000 (0%)]\tLoss: -796.552612\n",
      "Train Epoch: 151 [25600/60000 (43%)]\tLoss: -724.845459\n",
      "Train Epoch: 151 [51200/60000 (85%)]\tLoss: -762.397644\n",
      "\n",
      "Test set: Average loss: -595.8415\n",
      "\n",
      "Train Epoch: 152 [0/60000 (0%)]\tLoss: -718.940552\n",
      "Train Epoch: 152 [25600/60000 (43%)]\tLoss: -701.075073\n",
      "Train Epoch: 152 [51200/60000 (85%)]\tLoss: -633.742676\n",
      "\n",
      "Test set: Average loss: -553.7847\n",
      "\n",
      "Train Epoch: 153 [0/60000 (0%)]\tLoss: -684.622986\n",
      "Train Epoch: 153 [25600/60000 (43%)]\tLoss: -724.014954\n",
      "Train Epoch: 153 [51200/60000 (85%)]\tLoss: -733.566711\n",
      "\n",
      "Test set: Average loss: -620.0659\n",
      "\n",
      "Train Epoch: 154 [0/60000 (0%)]\tLoss: -625.166687\n",
      "Train Epoch: 154 [25600/60000 (43%)]\tLoss: -624.684265\n",
      "Train Epoch: 154 [51200/60000 (85%)]\tLoss: -596.267578\n",
      "\n",
      "Test set: Average loss: -615.1564\n",
      "\n",
      "Train Epoch: 155 [0/60000 (0%)]\tLoss: -671.104370\n",
      "Train Epoch: 155 [25600/60000 (43%)]\tLoss: -607.385986\n",
      "Train Epoch: 155 [51200/60000 (85%)]\tLoss: -671.672180\n",
      "\n",
      "Test set: Average loss: -604.6922\n",
      "\n",
      "Train Epoch: 156 [0/60000 (0%)]\tLoss: -583.477173\n",
      "Train Epoch: 156 [25600/60000 (43%)]\tLoss: -726.338196\n",
      "Train Epoch: 156 [51200/60000 (85%)]\tLoss: -734.060913\n",
      "\n",
      "Test set: Average loss: -607.2881\n",
      "\n",
      "Train Epoch: 157 [0/60000 (0%)]\tLoss: -751.259033\n",
      "Train Epoch: 157 [25600/60000 (43%)]\tLoss: -610.519653\n",
      "Train Epoch: 157 [51200/60000 (85%)]\tLoss: -729.179810\n",
      "\n",
      "Test set: Average loss: -619.6109\n",
      "\n",
      "Train Epoch: 158 [0/60000 (0%)]\tLoss: -811.406372\n",
      "Train Epoch: 158 [25600/60000 (43%)]\tLoss: -645.392151\n",
      "Train Epoch: 158 [51200/60000 (85%)]\tLoss: -678.336914\n",
      "\n",
      "Test set: Average loss: -589.8753\n",
      "\n",
      "Train Epoch: 159 [0/60000 (0%)]\tLoss: -719.622681\n",
      "Train Epoch: 159 [25600/60000 (43%)]\tLoss: -576.956116\n",
      "Train Epoch: 159 [51200/60000 (85%)]\tLoss: -831.013123\n",
      "\n",
      "Test set: Average loss: -597.2162\n",
      "\n",
      "Train Epoch: 160 [0/60000 (0%)]\tLoss: -532.292725\n",
      "Train Epoch: 160 [25600/60000 (43%)]\tLoss: -716.661621\n",
      "Train Epoch: 160 [51200/60000 (85%)]\tLoss: -703.078430\n",
      "\n",
      "Test set: Average loss: -614.5717\n",
      "\n",
      "Train Epoch: 161 [0/60000 (0%)]\tLoss: -689.456604\n",
      "Train Epoch: 161 [25600/60000 (43%)]\tLoss: -643.391113\n",
      "Train Epoch: 161 [51200/60000 (85%)]\tLoss: -271.186127\n",
      "\n",
      "Test set: Average loss: -622.6726\n",
      "\n",
      "Train Epoch: 162 [0/60000 (0%)]\tLoss: -683.889221\n",
      "Train Epoch: 162 [25600/60000 (43%)]\tLoss: -501.265961\n",
      "Train Epoch: 162 [51200/60000 (85%)]\tLoss: -641.940063\n",
      "\n",
      "Test set: Average loss: -610.0290\n",
      "\n",
      "Train Epoch: 163 [0/60000 (0%)]\tLoss: -656.076050\n",
      "Train Epoch: 163 [25600/60000 (43%)]\tLoss: -731.223083\n",
      "Train Epoch: 163 [51200/60000 (85%)]\tLoss: -660.398071\n",
      "\n",
      "Test set: Average loss: -556.3863\n",
      "\n",
      "Train Epoch: 164 [0/60000 (0%)]\tLoss: -745.712158\n",
      "Train Epoch: 164 [25600/60000 (43%)]\tLoss: -631.971497\n",
      "Train Epoch: 164 [51200/60000 (85%)]\tLoss: -727.341614\n",
      "\n",
      "Test set: Average loss: -570.5309\n",
      "\n",
      "Train Epoch: 165 [0/60000 (0%)]\tLoss: -688.501526\n",
      "Train Epoch: 165 [25600/60000 (43%)]\tLoss: -722.885620\n",
      "Train Epoch: 165 [51200/60000 (85%)]\tLoss: -561.665466\n",
      "\n",
      "Test set: Average loss: -619.0285\n",
      "\n",
      "Train Epoch: 166 [0/60000 (0%)]\tLoss: -639.845154\n",
      "Train Epoch: 166 [25600/60000 (43%)]\tLoss: -701.733887\n",
      "Train Epoch: 166 [51200/60000 (85%)]\tLoss: -803.081543\n",
      "\n",
      "Test set: Average loss: -599.6611\n",
      "\n",
      "Train Epoch: 167 [0/60000 (0%)]\tLoss: -717.601013\n",
      "Train Epoch: 167 [25600/60000 (43%)]\tLoss: -852.894287\n",
      "Train Epoch: 167 [51200/60000 (85%)]\tLoss: -788.569458\n",
      "\n",
      "Test set: Average loss: -577.1853\n",
      "\n",
      "Train Epoch: 168 [0/60000 (0%)]\tLoss: -779.908203\n",
      "Train Epoch: 168 [25600/60000 (43%)]\tLoss: -746.074768\n",
      "Train Epoch: 168 [51200/60000 (85%)]\tLoss: -713.900940\n",
      "\n",
      "Test set: Average loss: -589.0898\n",
      "\n",
      "Train Epoch: 169 [0/60000 (0%)]\tLoss: -667.593994\n",
      "Train Epoch: 169 [25600/60000 (43%)]\tLoss: -697.301392\n",
      "Train Epoch: 169 [51200/60000 (85%)]\tLoss: -739.226379\n",
      "\n",
      "Test set: Average loss: -614.0191\n",
      "\n",
      "Train Epoch: 170 [0/60000 (0%)]\tLoss: -775.970581\n",
      "Train Epoch: 170 [25600/60000 (43%)]\tLoss: -752.781311\n",
      "Train Epoch: 170 [51200/60000 (85%)]\tLoss: -750.022461\n",
      "\n",
      "Test set: Average loss: -614.8667\n",
      "\n",
      "Train Epoch: 171 [0/60000 (0%)]\tLoss: -617.453552\n",
      "Train Epoch: 171 [25600/60000 (43%)]\tLoss: -721.654419\n",
      "Train Epoch: 171 [51200/60000 (85%)]\tLoss: -679.874817\n",
      "\n",
      "Test set: Average loss: -567.9825\n",
      "\n",
      "Train Epoch: 172 [0/60000 (0%)]\tLoss: -745.621704\n",
      "Train Epoch: 172 [25600/60000 (43%)]\tLoss: -536.130981\n",
      "Train Epoch: 172 [51200/60000 (85%)]\tLoss: -665.828247\n",
      "\n",
      "Test set: Average loss: -627.1773\n",
      "\n",
      "Train Epoch: 173 [0/60000 (0%)]\tLoss: -794.660461\n",
      "Train Epoch: 173 [25600/60000 (43%)]\tLoss: -804.598145\n",
      "Train Epoch: 173 [51200/60000 (85%)]\tLoss: -777.350708\n",
      "\n",
      "Test set: Average loss: -610.2933\n",
      "\n",
      "Train Epoch: 174 [0/60000 (0%)]\tLoss: -810.808105\n",
      "Train Epoch: 174 [25600/60000 (43%)]\tLoss: -616.277527\n",
      "Train Epoch: 174 [51200/60000 (85%)]\tLoss: -691.605652\n",
      "\n",
      "Test set: Average loss: -556.2908\n",
      "\n",
      "Train Epoch: 175 [0/60000 (0%)]\tLoss: -649.199097\n",
      "Train Epoch: 175 [25600/60000 (43%)]\tLoss: -680.162292\n",
      "Train Epoch: 175 [51200/60000 (85%)]\tLoss: -596.404114\n",
      "\n",
      "Test set: Average loss: -616.5778\n",
      "\n",
      "Train Epoch: 176 [0/60000 (0%)]\tLoss: -751.875488\n",
      "Train Epoch: 176 [25600/60000 (43%)]\tLoss: -804.243652\n",
      "Train Epoch: 176 [51200/60000 (85%)]\tLoss: -685.749695\n",
      "\n",
      "Test set: Average loss: -604.3946\n",
      "\n",
      "Train Epoch: 177 [0/60000 (0%)]\tLoss: -752.354492\n",
      "Train Epoch: 177 [25600/60000 (43%)]\tLoss: -813.998291\n",
      "Train Epoch: 177 [51200/60000 (85%)]\tLoss: -780.861389\n",
      "\n",
      "Test set: Average loss: -613.9415\n",
      "\n",
      "Train Epoch: 178 [0/60000 (0%)]\tLoss: -835.453491\n",
      "Train Epoch: 178 [25600/60000 (43%)]\tLoss: -685.277954\n",
      "Train Epoch: 178 [51200/60000 (85%)]\tLoss: -672.762695\n",
      "\n",
      "Test set: Average loss: -630.1738\n",
      "\n",
      "Train Epoch: 179 [0/60000 (0%)]\tLoss: -795.929932\n",
      "Train Epoch: 179 [25600/60000 (43%)]\tLoss: -783.626160\n",
      "Train Epoch: 179 [51200/60000 (85%)]\tLoss: -578.101135\n",
      "\n",
      "Test set: Average loss: -632.0116\n",
      "\n",
      "Train Epoch: 180 [0/60000 (0%)]\tLoss: -766.789124\n",
      "Train Epoch: 180 [25600/60000 (43%)]\tLoss: -762.830627\n",
      "Train Epoch: 180 [51200/60000 (85%)]\tLoss: -700.591187\n",
      "\n",
      "Test set: Average loss: -598.9624\n",
      "\n",
      "Train Epoch: 181 [0/60000 (0%)]\tLoss: -591.536438\n",
      "Train Epoch: 181 [25600/60000 (43%)]\tLoss: -822.654785\n",
      "Train Epoch: 181 [51200/60000 (85%)]\tLoss: -808.476074\n",
      "\n",
      "Test set: Average loss: -574.1855\n",
      "\n",
      "Train Epoch: 182 [0/60000 (0%)]\tLoss: -764.777100\n",
      "Train Epoch: 182 [25600/60000 (43%)]\tLoss: -741.395386\n",
      "Train Epoch: 182 [51200/60000 (85%)]\tLoss: -756.375549\n",
      "\n",
      "Test set: Average loss: -577.1987\n",
      "\n",
      "Train Epoch: 183 [0/60000 (0%)]\tLoss: -715.689514\n",
      "Train Epoch: 183 [25600/60000 (43%)]\tLoss: -716.936340\n",
      "Train Epoch: 183 [51200/60000 (85%)]\tLoss: -671.080688\n",
      "\n",
      "Test set: Average loss: -619.8533\n",
      "\n",
      "Train Epoch: 184 [0/60000 (0%)]\tLoss: -824.878418\n",
      "Train Epoch: 184 [25600/60000 (43%)]\tLoss: -753.757019\n",
      "Train Epoch: 184 [51200/60000 (85%)]\tLoss: -730.245911\n",
      "\n",
      "Test set: Average loss: -633.2325\n",
      "\n",
      "Train Epoch: 185 [0/60000 (0%)]\tLoss: -609.639709\n",
      "Train Epoch: 185 [25600/60000 (43%)]\tLoss: -712.235352\n",
      "Train Epoch: 185 [51200/60000 (85%)]\tLoss: -709.086243\n",
      "\n",
      "Test set: Average loss: -630.9650\n",
      "\n",
      "Train Epoch: 186 [0/60000 (0%)]\tLoss: -808.274963\n",
      "Train Epoch: 186 [25600/60000 (43%)]\tLoss: -355.118011\n",
      "Train Epoch: 186 [51200/60000 (85%)]\tLoss: -779.216736\n",
      "\n",
      "Test set: Average loss: -633.0999\n",
      "\n",
      "Train Epoch: 187 [0/60000 (0%)]\tLoss: -727.766846\n",
      "Train Epoch: 187 [25600/60000 (43%)]\tLoss: -710.117920\n",
      "Train Epoch: 187 [51200/60000 (85%)]\tLoss: -768.016968\n",
      "\n",
      "Test set: Average loss: -608.2805\n",
      "\n",
      "Train Epoch: 188 [0/60000 (0%)]\tLoss: -512.194702\n",
      "Train Epoch: 188 [25600/60000 (43%)]\tLoss: -766.500610\n",
      "Train Epoch: 188 [51200/60000 (85%)]\tLoss: -707.397156\n",
      "\n",
      "Test set: Average loss: -639.3074\n",
      "\n",
      "Train Epoch: 189 [0/60000 (0%)]\tLoss: -783.002197\n",
      "Train Epoch: 189 [25600/60000 (43%)]\tLoss: -703.723389\n",
      "Train Epoch: 189 [51200/60000 (85%)]\tLoss: -666.785095\n",
      "\n",
      "Test set: Average loss: -620.9565\n",
      "\n",
      "Train Epoch: 190 [0/60000 (0%)]\tLoss: -625.079590\n",
      "Train Epoch: 190 [25600/60000 (43%)]\tLoss: -624.045166\n",
      "Train Epoch: 190 [51200/60000 (85%)]\tLoss: -747.377686\n",
      "\n",
      "Test set: Average loss: -645.4676\n",
      "\n",
      "Train Epoch: 191 [0/60000 (0%)]\tLoss: -772.522217\n",
      "Train Epoch: 191 [25600/60000 (43%)]\tLoss: -694.919922\n",
      "Train Epoch: 191 [51200/60000 (85%)]\tLoss: -708.805969\n",
      "\n",
      "Test set: Average loss: -624.9149\n",
      "\n",
      "Train Epoch: 192 [0/60000 (0%)]\tLoss: -725.078003\n",
      "Train Epoch: 192 [25600/60000 (43%)]\tLoss: -499.475555\n",
      "Train Epoch: 192 [51200/60000 (85%)]\tLoss: -693.434753\n",
      "\n",
      "Test set: Average loss: -650.4750\n",
      "\n",
      "Train Epoch: 193 [0/60000 (0%)]\tLoss: -588.001465\n",
      "Train Epoch: 193 [25600/60000 (43%)]\tLoss: -724.082642\n",
      "Train Epoch: 193 [51200/60000 (85%)]\tLoss: -477.109039\n",
      "\n",
      "Test set: Average loss: -583.1273\n",
      "\n",
      "Train Epoch: 194 [0/60000 (0%)]\tLoss: -410.057526\n",
      "Train Epoch: 194 [25600/60000 (43%)]\tLoss: -681.781128\n",
      "Train Epoch: 194 [51200/60000 (85%)]\tLoss: -769.303406\n",
      "\n",
      "Test set: Average loss: -620.8586\n",
      "\n",
      "Train Epoch: 195 [0/60000 (0%)]\tLoss: -804.215393\n",
      "Train Epoch: 195 [25600/60000 (43%)]\tLoss: -785.601562\n",
      "Train Epoch: 195 [51200/60000 (85%)]\tLoss: -823.685425\n",
      "\n",
      "Test set: Average loss: -628.3529\n",
      "\n",
      "Train Epoch: 196 [0/60000 (0%)]\tLoss: -487.868347\n",
      "Train Epoch: 196 [25600/60000 (43%)]\tLoss: -730.325928\n",
      "Train Epoch: 196 [51200/60000 (85%)]\tLoss: -766.997192\n",
      "\n",
      "Test set: Average loss: -639.7466\n",
      "\n",
      "Train Epoch: 197 [0/60000 (0%)]\tLoss: -650.731934\n",
      "Train Epoch: 197 [25600/60000 (43%)]\tLoss: -620.968689\n",
      "Train Epoch: 197 [51200/60000 (85%)]\tLoss: -498.932983\n",
      "\n",
      "Test set: Average loss: -603.9510\n",
      "\n",
      "Train Epoch: 198 [0/60000 (0%)]\tLoss: -836.818237\n",
      "Train Epoch: 198 [25600/60000 (43%)]\tLoss: -672.768005\n",
      "Train Epoch: 198 [51200/60000 (85%)]\tLoss: -636.792603\n",
      "\n",
      "Test set: Average loss: -565.6490\n",
      "\n",
      "Train Epoch: 199 [0/60000 (0%)]\tLoss: -789.559326\n",
      "Train Epoch: 199 [25600/60000 (43%)]\tLoss: -827.383911\n",
      "Train Epoch: 199 [51200/60000 (85%)]\tLoss: -823.230164\n",
      "\n",
      "Test set: Average loss: -617.4727\n",
      "\n",
      "Train Epoch: 200 [0/60000 (0%)]\tLoss: -789.094055\n",
      "Train Epoch: 200 [25600/60000 (43%)]\tLoss: -766.429626\n",
      "Train Epoch: 200 [51200/60000 (85%)]\tLoss: -687.450745\n",
      "\n",
      "Test set: Average loss: -629.8564\n",
      "\n",
      "Train Epoch: 201 [0/60000 (0%)]\tLoss: -892.900818\n",
      "Train Epoch: 201 [25600/60000 (43%)]\tLoss: -854.502930\n",
      "Train Epoch: 201 [51200/60000 (85%)]\tLoss: -852.779663\n",
      "\n",
      "Test set: Average loss: -629.0069\n",
      "\n",
      "Train Epoch: 202 [0/60000 (0%)]\tLoss: -810.940369\n",
      "Train Epoch: 202 [25600/60000 (43%)]\tLoss: -739.468994\n",
      "Train Epoch: 202 [51200/60000 (85%)]\tLoss: -780.244324\n",
      "\n",
      "Test set: Average loss: -603.9056\n",
      "\n",
      "Train Epoch: 203 [0/60000 (0%)]\tLoss: -816.197510\n",
      "Train Epoch: 203 [25600/60000 (43%)]\tLoss: -495.757385\n",
      "Train Epoch: 203 [51200/60000 (85%)]\tLoss: -809.541748\n",
      "\n",
      "Test set: Average loss: -628.5657\n",
      "\n",
      "Train Epoch: 204 [0/60000 (0%)]\tLoss: -681.023010\n",
      "Train Epoch: 204 [25600/60000 (43%)]\tLoss: -793.047913\n",
      "Train Epoch: 204 [51200/60000 (85%)]\tLoss: -673.466125\n",
      "\n",
      "Test set: Average loss: -637.2572\n",
      "\n",
      "Train Epoch: 205 [0/60000 (0%)]\tLoss: -730.502747\n",
      "Train Epoch: 205 [25600/60000 (43%)]\tLoss: -666.502747\n",
      "Train Epoch: 205 [51200/60000 (85%)]\tLoss: -763.358032\n",
      "\n",
      "Test set: Average loss: -624.8822\n",
      "\n",
      "Train Epoch: 206 [0/60000 (0%)]\tLoss: -776.118225\n",
      "Train Epoch: 206 [25600/60000 (43%)]\tLoss: -678.273376\n",
      "Train Epoch: 206 [51200/60000 (85%)]\tLoss: -725.600647\n",
      "\n",
      "Test set: Average loss: -621.2794\n",
      "\n",
      "Train Epoch: 207 [0/60000 (0%)]\tLoss: -877.931274\n",
      "Train Epoch: 207 [25600/60000 (43%)]\tLoss: -644.893127\n",
      "Train Epoch: 207 [51200/60000 (85%)]\tLoss: -623.689331\n",
      "\n",
      "Test set: Average loss: -621.0359\n",
      "\n",
      "Train Epoch: 208 [0/60000 (0%)]\tLoss: -816.288818\n",
      "Train Epoch: 208 [25600/60000 (43%)]\tLoss: -397.751617\n",
      "Train Epoch: 208 [51200/60000 (85%)]\tLoss: -872.048157\n",
      "\n",
      "Test set: Average loss: -596.7475\n",
      "\n",
      "Train Epoch: 209 [0/60000 (0%)]\tLoss: -811.585876\n",
      "Train Epoch: 209 [25600/60000 (43%)]\tLoss: -742.059937\n",
      "Train Epoch: 209 [51200/60000 (85%)]\tLoss: -501.850403\n",
      "\n",
      "Test set: Average loss: -667.8801\n",
      "\n",
      "Train Epoch: 210 [0/60000 (0%)]\tLoss: -787.357544\n",
      "Train Epoch: 210 [25600/60000 (43%)]\tLoss: -744.555359\n",
      "Train Epoch: 210 [51200/60000 (85%)]\tLoss: -542.167969\n",
      "\n",
      "Test set: Average loss: -622.4220\n",
      "\n",
      "Train Epoch: 211 [0/60000 (0%)]\tLoss: -737.136475\n",
      "Train Epoch: 211 [25600/60000 (43%)]\tLoss: -814.843872\n",
      "Train Epoch: 211 [51200/60000 (85%)]\tLoss: -888.645203\n",
      "\n",
      "Test set: Average loss: -630.1681\n",
      "\n",
      "Train Epoch: 212 [0/60000 (0%)]\tLoss: -858.012268\n",
      "Train Epoch: 212 [25600/60000 (43%)]\tLoss: -794.090942\n",
      "Train Epoch: 212 [51200/60000 (85%)]\tLoss: -579.416321\n",
      "\n",
      "Test set: Average loss: -618.6804\n",
      "\n",
      "Train Epoch: 213 [0/60000 (0%)]\tLoss: -771.849487\n",
      "Train Epoch: 213 [25600/60000 (43%)]\tLoss: -804.900024\n",
      "Train Epoch: 213 [51200/60000 (85%)]\tLoss: -675.916626\n",
      "\n",
      "Test set: Average loss: -607.6193\n",
      "\n",
      "Train Epoch: 214 [0/60000 (0%)]\tLoss: -750.915771\n",
      "Train Epoch: 214 [25600/60000 (43%)]\tLoss: -251.604660\n",
      "Train Epoch: 214 [51200/60000 (85%)]\tLoss: -699.675232\n",
      "\n",
      "Test set: Average loss: -647.5418\n",
      "\n",
      "Train Epoch: 215 [0/60000 (0%)]\tLoss: -829.439880\n",
      "Train Epoch: 215 [25600/60000 (43%)]\tLoss: -776.904907\n",
      "Train Epoch: 215 [51200/60000 (85%)]\tLoss: -617.776001\n",
      "\n",
      "Test set: Average loss: -647.2995\n",
      "\n",
      "Train Epoch: 216 [0/60000 (0%)]\tLoss: -747.616272\n",
      "Train Epoch: 216 [25600/60000 (43%)]\tLoss: -757.749512\n",
      "Train Epoch: 216 [51200/60000 (85%)]\tLoss: -688.006897\n",
      "\n",
      "Test set: Average loss: -628.3602\n",
      "\n",
      "Train Epoch: 217 [0/60000 (0%)]\tLoss: -751.833191\n",
      "Train Epoch: 217 [25600/60000 (43%)]\tLoss: -717.948059\n",
      "Train Epoch: 217 [51200/60000 (85%)]\tLoss: -810.878906\n",
      "\n",
      "Test set: Average loss: -624.2265\n",
      "\n",
      "Train Epoch: 218 [0/60000 (0%)]\tLoss: -604.742004\n",
      "Train Epoch: 218 [25600/60000 (43%)]\tLoss: -707.129395\n",
      "Train Epoch: 218 [51200/60000 (85%)]\tLoss: -532.676270\n",
      "\n",
      "Test set: Average loss: -586.1409\n",
      "\n",
      "Train Epoch: 219 [0/60000 (0%)]\tLoss: -753.431763\n",
      "Train Epoch: 219 [25600/60000 (43%)]\tLoss: -843.201050\n",
      "Train Epoch: 219 [51200/60000 (85%)]\tLoss: -834.905273\n",
      "\n",
      "Test set: Average loss: -613.8172\n",
      "\n",
      "Train Epoch: 220 [0/60000 (0%)]\tLoss: -906.227051\n",
      "Train Epoch: 220 [25600/60000 (43%)]\tLoss: -691.059265\n",
      "Train Epoch: 220 [51200/60000 (85%)]\tLoss: -773.327820\n",
      "\n",
      "Test set: Average loss: -631.0559\n",
      "\n",
      "Train Epoch: 221 [0/60000 (0%)]\tLoss: -830.575378\n",
      "Train Epoch: 221 [25600/60000 (43%)]\tLoss: -807.256104\n",
      "Train Epoch: 221 [51200/60000 (85%)]\tLoss: -564.015015\n",
      "\n",
      "Test set: Average loss: -643.1199\n",
      "\n",
      "Train Epoch: 222 [0/60000 (0%)]\tLoss: -925.265991\n",
      "Train Epoch: 222 [25600/60000 (43%)]\tLoss: -880.611694\n",
      "Train Epoch: 222 [51200/60000 (85%)]\tLoss: -791.119202\n",
      "\n",
      "Test set: Average loss: -632.0887\n",
      "\n",
      "Train Epoch: 223 [0/60000 (0%)]\tLoss: -729.669006\n",
      "Train Epoch: 223 [25600/60000 (43%)]\tLoss: -507.319824\n",
      "Train Epoch: 223 [51200/60000 (85%)]\tLoss: -684.976440\n",
      "\n",
      "Test set: Average loss: -635.3444\n",
      "\n",
      "Train Epoch: 224 [0/60000 (0%)]\tLoss: -646.937256\n",
      "Train Epoch: 224 [25600/60000 (43%)]\tLoss: -645.568420\n",
      "Train Epoch: 224 [51200/60000 (85%)]\tLoss: -780.822449\n",
      "\n",
      "Test set: Average loss: -622.1482\n",
      "\n",
      "Train Epoch: 225 [0/60000 (0%)]\tLoss: -805.842346\n",
      "Train Epoch: 225 [25600/60000 (43%)]\tLoss: -631.934570\n",
      "Train Epoch: 225 [51200/60000 (85%)]\tLoss: -602.335571\n",
      "\n",
      "Test set: Average loss: -622.4624\n",
      "\n",
      "Train Epoch: 226 [0/60000 (0%)]\tLoss: -695.414795\n",
      "Train Epoch: 226 [25600/60000 (43%)]\tLoss: -763.919800\n",
      "Train Epoch: 226 [51200/60000 (85%)]\tLoss: -759.405090\n",
      "\n",
      "Test set: Average loss: -637.2820\n",
      "\n",
      "Train Epoch: 227 [0/60000 (0%)]\tLoss: -694.221863\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 500\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('./mnist/', transform=transform)\n",
    "validation_data = datasets.MNIST('./mnist/', train=False,\n",
    "                                 transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    validation_data,\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adagrad([\n",
    "    {'params': model.encoder.parameters()},\n",
    "    {'params': model.decoder.parameters(), 'weight_decay': 0.1}\n",
    "], lr=0.001)\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.normal_(m.weight, 0.0, 0.01)\n",
    "        init.normal_(m.bias, 0.0, 0.01)\n",
    "\n",
    "model.apply(weights_init)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.distributions.Normal(loc=torch.zeros((32,)), scale=torch.ones((32,)))\n",
    "(0.1307,), (0.3081,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.reshape(model.decoder(z.sample().to(device)).sample(), (28,28)) * .3081 + 0.1307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(30,10))\n",
    "for row in axs:\n",
    "    for ax in row:\n",
    "        ax.imshow(x.cpu(), cmap='Greys', norm=colors.Normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,  2.8215,  2.1087,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  0.6704,  2.7706,  2.7451,  0.5431,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.1696,  2.5797,  2.7960,  2.1087, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242,  1.2050,  2.7960,  2.7578,  0.5431, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  0.9123,  2.7069,  2.7960,  0.8995, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242,  2.0578,  2.7960,  2.7960, -0.3606, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1187,\n",
       "           2.2105,  2.7706,  2.7324,  1.3832, -0.4115, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1060,  2.2742,\n",
       "           2.7960,  2.7069,  0.0467, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.3733,  2.2742,  2.7960,\n",
       "           2.6815,  0.7595, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  0.3649,  2.7960,  2.7960,\n",
       "           2.2233, -0.2460, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242,  1.2686,  2.7960,  2.7960,\n",
       "           1.7650, -0.2460, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242,  0.0849,  2.7197,  2.7960,  2.3505,\n",
       "          -0.2587, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242,  0.0849,  2.4651,  2.7960,  2.7960,  0.9123,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.3733,  1.5105,  2.7960,  2.7960,  1.7269, -0.2969,\n",
       "          -0.2715,  0.8995,  1.7905,  2.7451,  2.6687,  0.3649, -0.2715,\n",
       "          -0.3733, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.0169,  1.8287,  2.7960,  2.7960,  1.7396,  2.0451,  2.2996,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           0.6450, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.2631,  2.7960,  2.7960,  1.9051,  0.6959,  2.7960,  2.7960,\n",
       "           2.6560,  1.7396,  1.2559,  1.5232,  1.4850,  2.7960,  2.7960,\n",
       "           1.9687, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           0.2631,  2.7960,  2.7960,  1.7905,  1.5741,  2.7960,  2.6687,\n",
       "           0.7213, -0.4242, -0.4242, -0.4242, -0.3351,  2.1723,  2.7960,\n",
       "           2.3633, -0.1060, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.2842,  1.6632,  2.7960,  2.7960,  2.7960,  2.7960,  2.4396,\n",
       "           0.6450, -0.1696, -0.0169,  0.1740,  0.6450,  2.1723,  2.7960,\n",
       "           2.3633, -0.1060, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.2969,  1.6887,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "           2.7960,  2.4651,  2.5287,  2.6051,  2.7960,  2.7960,  2.5542,\n",
       "           0.3268, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.3478,  0.4286,  1.3196,  2.7960,  2.7960,\n",
       "           2.7960,  2.7960,  2.7960,  2.7960,  2.7324,  1.1286, -0.2460,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_loader.__iter__().next()[0][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_index_sampler',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'pin_memory',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder', Encoder(\n",
      "  (dense): Linear(in_features=784, out_features=500, bias=True)\n",
      "  (mean): Linear(in_features=500, out_features=32, bias=True)\n",
      "  (log_var): Linear(in_features=500, out_features=32, bias=True)\n",
      "))\n",
      "('decoder', GaussianDecoder(\n",
      "  (dense): Linear(in_features=32, out_features=500, bias=True)\n",
      "  (mean): Linear(in_features=500, out_features=784, bias=True)\n",
      "  (log_var): Linear(in_features=500, out_features=784, bias=True)\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "for m in model.named_children():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.dense.weight\n",
      "torch.Size([500, 784])\n",
      "Parameter containing:\n",
      "tensor([[-0.0064,  0.0120, -0.0143,  ..., -0.0038,  0.0143, -0.0100],\n",
      "        [-0.0093, -0.0072, -0.0008,  ..., -0.0013, -0.0177, -0.0073],\n",
      "        [ 0.0086, -0.0201, -0.0062,  ...,  0.0021,  0.0043,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0036,  0.0036, -0.0075,  ...,  0.0051,  0.0180, -0.0051],\n",
      "        [-0.0066, -0.0075, -0.0079,  ..., -0.0022,  0.0040,  0.0058],\n",
      "        [ 0.0076, -0.0049, -0.0008,  ..., -0.0308, -0.0187, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.dense.bias\n",
      "torch.Size([500])\n",
      "Parameter containing:\n",
      "tensor([-1.7255e-02,  1.2876e-03, -1.7940e-02, -5.5748e-03, -8.5161e-03,\n",
      "         9.3608e-03, -3.8954e-03,  9.1603e-03, -7.9353e-04, -5.5534e-03,\n",
      "        -5.0324e-03, -5.5355e-03,  9.8530e-03, -1.0357e-02, -1.2586e-02,\n",
      "         1.6232e-02,  1.0596e-02, -1.1701e-02, -1.4952e-03, -1.0913e-02,\n",
      "         3.9395e-03,  1.2993e-02, -3.6659e-03, -9.8283e-04, -1.2446e-02,\n",
      "        -1.2244e-02,  2.4599e-03,  1.6911e-02,  8.9535e-03,  9.1543e-03,\n",
      "        -1.7371e-02, -1.8100e-03,  7.9773e-03,  6.8443e-03,  2.7356e-02,\n",
      "         1.5751e-03, -5.6750e-03,  7.7189e-03,  2.0719e-03,  8.9749e-03,\n",
      "         1.0029e-02,  2.1326e-02,  1.3170e-02,  3.8187e-03,  1.0186e-02,\n",
      "        -1.1212e-02, -2.3334e-03, -4.4433e-03,  2.6100e-02, -1.6681e-02,\n",
      "         1.2806e-02,  3.0607e-03,  2.8412e-02,  5.7120e-03, -2.6694e-02,\n",
      "         9.0752e-04,  1.1270e-02, -6.1237e-04,  2.0534e-02,  1.0386e-03,\n",
      "         1.2787e-03,  5.2276e-03,  3.4981e-03,  2.6404e-04,  5.4307e-03,\n",
      "         9.3697e-03,  2.5373e-03, -7.7172e-03, -1.7935e-02, -1.4050e-02,\n",
      "         2.7298e-03, -6.3592e-05, -4.4313e-03,  2.3857e-02,  2.1310e-02,\n",
      "         1.1775e-02, -2.0738e-03,  2.5415e-03,  9.7528e-04,  4.3937e-03,\n",
      "        -1.6003e-02, -6.0223e-03, -1.3471e-02, -6.7378e-03, -1.6412e-03,\n",
      "         1.4761e-02,  3.9835e-03, -8.7572e-03,  4.6661e-04, -5.1822e-04,\n",
      "        -1.5053e-03, -1.1362e-02, -5.1723e-03,  2.1225e-02,  4.8066e-03,\n",
      "         1.3993e-02, -1.3715e-02, -5.3930e-03,  4.2916e-03,  9.4937e-03,\n",
      "         2.2386e-03,  2.9860e-03,  1.3691e-02, -4.4437e-03, -3.2687e-03,\n",
      "         4.7082e-03,  9.0814e-03,  1.6466e-02, -7.3627e-03, -8.6549e-04,\n",
      "        -1.9718e-02, -2.1572e-02, -1.0600e-02,  5.3795e-03,  1.9640e-02,\n",
      "         1.0791e-02, -5.6941e-03, -1.3681e-02,  4.3963e-03, -1.4309e-02,\n",
      "        -2.6319e-03,  1.1564e-02, -8.7014e-03,  1.5990e-03,  1.0381e-02,\n",
      "        -1.1689e-02, -9.3705e-03, -3.8958e-03,  2.1670e-02,  3.0341e-03,\n",
      "        -2.5751e-03,  4.9041e-04,  1.5000e-02, -2.2843e-03,  8.4060e-03,\n",
      "        -1.6796e-02, -1.2692e-03,  1.7139e-02,  1.0765e-02, -2.0494e-03,\n",
      "         2.7384e-03,  8.8313e-03,  2.5329e-03,  7.3217e-03, -8.3021e-03,\n",
      "        -1.1835e-03, -1.0905e-02,  2.9688e-05, -8.8600e-03,  5.1900e-03,\n",
      "         6.5420e-03, -7.3522e-03,  8.0465e-03,  1.2113e-02,  3.0632e-03,\n",
      "         1.5496e-02,  1.5659e-03, -1.4562e-02,  1.1433e-02, -8.3807e-03,\n",
      "         1.2120e-02, -1.2723e-02,  2.4091e-02,  3.4113e-03, -1.9224e-03,\n",
      "         1.5923e-03,  5.6676e-03,  8.2027e-03,  7.9792e-03, -6.7316e-03,\n",
      "        -2.4256e-02,  1.6015e-02,  2.7212e-02,  1.0669e-02,  1.6493e-02,\n",
      "        -8.8300e-03, -4.6697e-03, -1.7454e-03, -2.7777e-03, -3.4184e-03,\n",
      "        -5.0460e-04,  2.6318e-03, -7.7395e-03, -1.7289e-02, -2.1125e-02,\n",
      "         5.0480e-03,  9.0989e-03, -1.0710e-02,  1.4890e-03,  1.2151e-02,\n",
      "         1.1957e-03, -4.0249e-03,  3.5073e-03,  6.2708e-04, -7.3126e-03,\n",
      "        -6.7772e-03, -1.6651e-02,  9.7704e-03,  8.4218e-03, -2.6054e-02,\n",
      "         4.6025e-03, -1.0357e-02,  6.0858e-03, -3.4617e-03,  1.9104e-03,\n",
      "         1.4689e-02, -1.2743e-02,  1.4439e-03, -3.6159e-04, -2.3483e-03,\n",
      "         9.1767e-03,  1.5937e-02,  8.1073e-03,  6.6247e-03, -1.4319e-02,\n",
      "        -2.3182e-02,  6.2089e-03,  6.3042e-03,  3.8662e-03, -5.5840e-03,\n",
      "         6.4921e-04, -1.7167e-02, -7.6936e-03, -1.3076e-02, -6.7619e-03,\n",
      "        -5.8535e-04, -1.6175e-02,  1.5491e-02, -1.4356e-03, -1.7882e-02,\n",
      "        -1.4114e-02,  2.0145e-03,  5.4802e-03,  6.0191e-03,  1.0013e-02,\n",
      "        -1.6250e-02, -1.9760e-02, -1.9982e-03,  2.2590e-03, -5.9712e-03,\n",
      "         4.4854e-03, -5.9924e-03, -1.0829e-02, -1.1390e-02, -2.1853e-02,\n",
      "        -5.4945e-03, -1.4137e-02,  8.6667e-03, -1.4526e-02, -5.7093e-03,\n",
      "         1.7251e-03,  1.3101e-02,  1.0120e-02, -1.6466e-02,  1.4757e-03,\n",
      "        -1.2864e-02,  3.7424e-03, -6.1097e-03, -5.8512e-03,  1.2874e-02,\n",
      "         5.4664e-04, -4.0908e-03,  4.8353e-03, -6.2024e-04, -1.1185e-02,\n",
      "         2.3060e-03, -8.4516e-03, -1.3201e-03,  9.3039e-03, -2.3312e-02,\n",
      "         3.0271e-03, -6.8766e-03, -1.6382e-02,  9.8223e-03,  2.2907e-03,\n",
      "         2.0603e-03, -8.5393e-03, -1.5022e-03,  5.2455e-03,  7.0277e-03,\n",
      "         2.9864e-03, -1.6557e-02, -7.3370e-03,  1.2812e-02,  2.9758e-03,\n",
      "         3.2330e-03, -2.2056e-02,  4.7603e-03, -1.3455e-02, -4.3492e-03,\n",
      "        -7.9248e-03, -1.5728e-02, -8.6170e-03, -1.8585e-02,  1.4173e-03,\n",
      "        -1.0812e-03, -2.7285e-04,  7.8128e-03,  4.2235e-03, -1.7772e-03,\n",
      "        -1.8297e-03, -9.0491e-03,  8.4948e-03, -8.3289e-03, -1.7789e-02,\n",
      "         7.5292e-04, -1.0519e-02,  1.6520e-03,  4.1581e-03, -1.7285e-03,\n",
      "         1.3586e-03,  3.5896e-03,  4.4367e-03,  4.1659e-03, -2.1226e-02,\n",
      "        -1.3155e-02,  5.2192e-03,  1.1578e-02, -7.3229e-03, -6.6980e-03,\n",
      "        -1.6536e-03, -1.1356e-02,  1.2043e-02,  1.9704e-02,  4.6422e-03,\n",
      "         2.0148e-02, -1.9476e-02, -1.7042e-02,  1.4237e-02,  9.3032e-04,\n",
      "         5.6563e-04, -8.7032e-03,  3.0199e-03,  2.1016e-02,  1.9003e-03,\n",
      "         3.9115e-03, -5.0519e-03,  2.9937e-03, -8.6768e-03,  4.9652e-03,\n",
      "         2.0076e-03,  8.5182e-03,  1.5090e-02,  2.1963e-03,  5.3000e-03,\n",
      "        -2.4452e-03,  2.5585e-03, -2.1136e-03,  1.1706e-02,  9.6351e-03,\n",
      "        -9.6808e-03,  5.9949e-03, -4.8690e-03,  5.8362e-03,  2.3273e-03,\n",
      "        -6.1033e-03, -1.1395e-02, -1.1905e-03, -3.3747e-03,  6.4756e-03,\n",
      "         3.6609e-03,  8.6627e-03, -1.3550e-03,  1.0469e-02, -3.2357e-03,\n",
      "        -3.7771e-03, -4.3076e-03,  2.2780e-02, -1.5004e-02,  1.7040e-03,\n",
      "         7.1673e-03,  1.2685e-02, -1.6022e-03,  2.3628e-03, -7.2228e-04,\n",
      "         3.2040e-03, -1.4388e-02,  8.4491e-03, -1.0531e-03,  1.9969e-02,\n",
      "        -7.6114e-03, -9.0445e-03, -1.0795e-02, -5.2468e-04, -9.9760e-03,\n",
      "         3.7290e-03, -4.8504e-03,  1.3669e-02, -9.5381e-03, -1.4884e-02,\n",
      "        -1.0248e-02,  6.4330e-03,  2.4908e-03,  3.3748e-04, -1.5558e-02,\n",
      "         7.7128e-03, -2.6640e-03, -3.3509e-03, -2.8554e-02,  1.7958e-03,\n",
      "         1.4003e-03, -9.4102e-03, -1.1286e-02, -1.0850e-02, -8.2588e-03,\n",
      "        -8.6581e-03, -1.8452e-02, -1.4489e-02, -2.4301e-02,  4.7523e-03,\n",
      "        -4.9758e-03,  4.7334e-03, -5.3829e-03, -4.5610e-03, -1.6315e-03,\n",
      "        -1.3004e-02,  1.3810e-02,  2.9241e-03,  2.0244e-03,  1.9856e-02,\n",
      "         4.5971e-04, -1.0800e-02,  4.4346e-03,  5.9359e-04, -1.0105e-02,\n",
      "        -9.5580e-03,  3.6552e-03, -7.9803e-04,  3.3325e-03, -4.9867e-03,\n",
      "         2.6268e-03,  1.1865e-02,  1.9714e-03, -1.8792e-02, -1.3602e-02,\n",
      "         1.7539e-02, -6.3831e-03, -4.2359e-03,  1.3809e-02,  1.0045e-02,\n",
      "        -8.7373e-03, -1.5800e-03, -1.3643e-02, -8.0585e-03, -3.9862e-03,\n",
      "         2.0908e-03,  6.7586e-03, -4.1888e-03, -5.2964e-03, -9.8211e-03,\n",
      "        -8.0362e-04,  5.8845e-03,  9.5371e-03,  7.5991e-03,  2.6358e-03,\n",
      "        -1.8278e-02, -1.1677e-02, -3.8163e-04,  3.2782e-03, -2.2870e-03,\n",
      "         1.3341e-02, -6.4767e-03,  1.2145e-03,  3.5995e-03, -8.6396e-03,\n",
      "         1.1731e-03,  8.1332e-04, -3.4786e-04,  1.7091e-02, -2.7160e-04,\n",
      "         9.1019e-03, -3.2756e-03,  2.0325e-02,  8.3569e-03, -9.9929e-03,\n",
      "        -1.2164e-02,  5.2821e-03, -5.1243e-04,  9.6019e-03, -1.3992e-04,\n",
      "        -1.6342e-02, -5.0467e-03,  6.3111e-03,  7.9870e-03, -6.5568e-03,\n",
      "         6.0925e-03, -2.0660e-04, -1.0898e-02,  1.0778e-02,  2.6590e-03,\n",
      "        -1.3723e-04, -3.5333e-05, -8.5351e-03, -1.8470e-02, -4.2599e-03,\n",
      "         7.7369e-04,  3.4125e-03, -1.1665e-02, -2.8307e-03, -2.6631e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.mean.weight\n",
      "torch.Size([32, 500])\n",
      "Parameter containing:\n",
      "tensor([[-0.0116,  0.0116, -0.0020,  ...,  0.0025, -0.0102, -0.0033],\n",
      "        [ 0.0116, -0.0107, -0.0153,  ..., -0.0161,  0.0066,  0.0010],\n",
      "        [ 0.0054,  0.0013, -0.0020,  ..., -0.0018, -0.0060, -0.0093],\n",
      "        ...,\n",
      "        [-0.0171,  0.0049, -0.0062,  ...,  0.0177, -0.0056,  0.0024],\n",
      "        [ 0.0016, -0.0086, -0.0061,  ...,  0.0118,  0.0055, -0.0031],\n",
      "        [-0.0012, -0.0118,  0.0063,  ...,  0.0069, -0.0069, -0.0026]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.mean.bias\n",
      "torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.0114, -0.0077,  0.0015,  0.0123, -0.0044, -0.0145, -0.0225, -0.0020,\n",
      "        -0.0048,  0.0032,  0.0118, -0.0261,  0.0145, -0.0093,  0.0057,  0.0018,\n",
      "        -0.0016, -0.0058, -0.0142,  0.0131,  0.0054, -0.0032,  0.0056,  0.0161,\n",
      "        -0.0016,  0.0164,  0.0009,  0.0048,  0.0181,  0.0066, -0.0121, -0.0003],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "encoder.log_var.weight\n",
      "torch.Size([32, 500])\n",
      "Parameter containing:\n",
      "tensor([[ 5.5247e-03,  7.2940e-03,  1.7442e-02,  ...,  2.1694e-03,\n",
      "          1.4990e-02, -1.4783e-03],\n",
      "        [ 1.8058e-02,  5.2937e-03, -5.6629e-03,  ..., -1.3743e-02,\n",
      "         -9.6719e-03, -1.8850e-02],\n",
      "        [ 6.5825e-03,  1.8317e-02,  6.5322e-03,  ...,  4.2150e-03,\n",
      "         -8.7071e-03, -9.0374e-03],\n",
      "        ...,\n",
      "        [-5.4529e-03,  6.3018e-03, -1.6203e-02,  ...,  6.4860e-03,\n",
      "         -1.5038e-03,  1.3862e-02],\n",
      "        [ 1.4798e-02, -6.1274e-03, -6.2464e-03,  ...,  5.3635e-03,\n",
      "          2.1422e-02,  1.1628e-02],\n",
      "        [ 2.3829e-03,  1.1055e-02, -6.0631e-05,  ...,  5.5732e-03,\n",
      "         -9.6535e-03, -7.0037e-03]], device='cuda:0', requires_grad=True)\n",
      "encoder.log_var.bias\n",
      "torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 2.0326e-02, -2.8721e-04,  1.4865e-02, -1.9070e-02,  1.4835e-02,\n",
      "        -2.6945e-03,  3.8184e-03, -1.7276e-02, -3.8420e-03, -2.5459e-02,\n",
      "        -2.4954e-03, -9.5328e-03, -8.8401e-05,  1.1252e-02,  1.4572e-02,\n",
      "        -4.2485e-04, -7.1088e-03, -1.6420e-02, -4.3165e-03, -8.5373e-03,\n",
      "         4.2524e-03, -1.0998e-02, -3.8176e-03,  1.9973e-03,  1.1695e-02,\n",
      "        -8.3233e-04,  1.7465e-02,  1.0736e-02, -1.4257e-02,  4.7023e-03,\n",
      "        -9.1872e-03, -6.7090e-03], device='cuda:0', requires_grad=True)\n",
      "decoder.dense.weight\n",
      "torch.Size([500, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.0112, -0.0131,  0.0131,  ...,  0.0060, -0.0097, -0.0138],\n",
      "        [-0.0128, -0.0124, -0.0072,  ...,  0.0134, -0.0193,  0.0015],\n",
      "        [-0.0049, -0.0181, -0.0050,  ...,  0.0021, -0.0052, -0.0031],\n",
      "        ...,\n",
      "        [-0.0114,  0.0019, -0.0008,  ...,  0.0065, -0.0162,  0.0193],\n",
      "        [-0.0110, -0.0020,  0.0147,  ...,  0.0056, -0.0206,  0.0028],\n",
      "        [-0.0080,  0.0262,  0.0063,  ...,  0.0078,  0.0125, -0.0020]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.dense.bias\n",
      "torch.Size([500])\n",
      "Parameter containing:\n",
      "tensor([-2.7025e-04, -2.0936e-02,  3.2521e-03, -1.2208e-02,  1.1571e-02,\n",
      "        -4.4196e-03,  4.4243e-03,  5.1933e-03,  1.6169e-02,  1.3814e-03,\n",
      "        -5.4173e-03,  1.2783e-04,  1.8765e-02,  1.8550e-03,  1.0101e-02,\n",
      "         1.5162e-02,  3.1648e-03, -1.2633e-02,  1.3734e-02,  7.2669e-03,\n",
      "         1.3484e-02, -2.0187e-02,  1.5069e-02, -9.9346e-03,  2.1897e-03,\n",
      "        -3.1665e-03, -1.3404e-03, -4.6859e-03, -6.1471e-03, -5.1529e-03,\n",
      "         6.2555e-03, -3.8979e-03,  6.4836e-03, -1.1928e-02,  1.5643e-02,\n",
      "        -4.3348e-03,  1.4668e-02, -1.8828e-02, -9.5556e-03, -7.0298e-03,\n",
      "        -4.6664e-03, -1.5186e-03,  4.7760e-03, -3.0364e-03,  1.8398e-03,\n",
      "         1.1793e-02,  8.9214e-03, -9.4471e-03,  5.0126e-04, -1.1712e-02,\n",
      "        -1.3763e-04, -3.3694e-03, -8.9865e-03,  7.7142e-03, -7.2991e-03,\n",
      "         4.0478e-03, -1.1321e-02,  8.5842e-03, -4.5014e-03, -1.4243e-02,\n",
      "        -4.2582e-03, -5.5093e-04,  3.8025e-03,  5.2831e-03,  3.6330e-03,\n",
      "        -1.8448e-02, -8.4476e-03,  5.7924e-03,  3.7749e-03, -1.3591e-02,\n",
      "         5.8673e-03,  9.3594e-03,  3.7740e-02, -6.0272e-03,  1.9563e-02,\n",
      "         7.5216e-03, -4.3592e-03, -3.0573e-03,  8.0004e-03,  4.1705e-03,\n",
      "         7.6161e-03,  6.1737e-03,  4.7028e-03,  1.8651e-02,  1.5154e-03,\n",
      "        -7.6479e-04, -1.0251e-02, -4.8389e-03, -2.7731e-03, -6.8442e-03,\n",
      "         8.3914e-03, -1.5640e-02, -5.6570e-03, -9.1956e-03, -2.6643e-04,\n",
      "        -7.3455e-03, -5.4576e-03,  1.4682e-02, -4.4044e-03, -4.0821e-03,\n",
      "        -1.9814e-02, -8.3731e-03, -2.0497e-03, -1.7433e-03, -3.6042e-03,\n",
      "         3.5493e-03, -1.9830e-02,  8.6765e-03, -2.4544e-02, -4.9296e-03,\n",
      "        -8.0420e-03, -5.6594e-03,  2.4188e-03, -4.8919e-03, -9.7271e-03,\n",
      "        -2.7021e-03,  1.9718e-02,  2.6561e-02, -5.7760e-03,  1.3330e-03,\n",
      "        -1.4022e-02, -5.4934e-03,  5.7621e-03, -1.9590e-03,  4.4231e-03,\n",
      "        -2.9802e-04,  2.0471e-03, -7.9765e-03,  5.6638e-03,  8.5887e-03,\n",
      "        -4.0734e-04, -3.6484e-03, -1.6705e-02,  1.5694e-02,  9.4871e-03,\n",
      "        -2.7383e-03,  4.9830e-03,  7.8845e-03,  6.9161e-03, -1.1008e-03,\n",
      "        -1.7839e-03,  4.6693e-03,  1.6951e-02,  1.4577e-02,  3.1046e-03,\n",
      "         1.5461e-02,  6.8978e-03,  8.3835e-03,  1.2118e-03, -2.3376e-03,\n",
      "        -7.2076e-03, -1.2181e-02, -8.9093e-03, -5.0451e-03, -5.7979e-03,\n",
      "         4.6443e-03,  1.9583e-02, -3.2788e-04,  8.4778e-03,  1.6889e-02,\n",
      "        -3.5160e-03, -2.7426e-03,  7.7071e-03,  4.4994e-03,  9.9788e-03,\n",
      "        -1.1316e-03,  1.7870e-02, -1.0669e-02, -7.3336e-03, -1.3631e-02,\n",
      "        -1.8132e-02, -1.4827e-02, -8.6128e-03, -1.5701e-02,  2.4284e-03,\n",
      "         6.7604e-03, -4.5972e-03, -1.4009e-03,  1.7281e-03,  1.2864e-02,\n",
      "         7.6803e-03, -8.0612e-03,  4.2503e-03,  1.9720e-04,  5.5865e-03,\n",
      "        -2.0787e-02, -7.8461e-03,  2.2167e-02,  1.3206e-03, -9.5260e-03,\n",
      "        -9.1408e-04,  1.3454e-02, -1.7970e-02, -4.1079e-03,  8.4555e-03,\n",
      "         1.2366e-03, -3.2719e-02,  5.3710e-04,  1.4374e-02,  5.5021e-05,\n",
      "        -3.8783e-03,  1.8447e-02, -6.4085e-03,  9.4276e-03, -6.4840e-04,\n",
      "        -2.7000e-03,  7.2949e-03,  1.4354e-02,  7.8273e-03, -4.3365e-03,\n",
      "         4.3781e-03, -7.5104e-03, -1.0925e-02,  6.8960e-03,  1.8959e-03,\n",
      "         2.6709e-03,  8.6038e-03, -9.8814e-03, -1.9394e-02,  3.0809e-02,\n",
      "        -2.0495e-03, -8.1345e-03,  1.1190e-03,  9.1772e-03,  8.8902e-03,\n",
      "        -1.8668e-02,  1.5927e-02, -5.0136e-03,  9.5483e-03,  8.2806e-03,\n",
      "         9.0492e-03,  6.5520e-03,  3.2579e-03, -1.9456e-02,  1.2178e-02,\n",
      "        -9.4336e-03, -2.4820e-03, -1.1853e-02,  1.7829e-02, -2.4817e-03,\n",
      "         2.4976e-02,  1.4074e-02, -8.5462e-03,  1.9669e-02, -4.8262e-03,\n",
      "        -5.8957e-03, -1.2679e-02,  9.9238e-03, -1.0454e-04, -1.4257e-02,\n",
      "        -1.8120e-02, -1.2990e-02, -1.0891e-02,  5.4624e-03, -3.4763e-03,\n",
      "        -7.8793e-03,  8.6149e-03,  4.4330e-03,  1.2447e-02, -2.5380e-02,\n",
      "         1.7006e-03, -1.4632e-02,  2.6368e-03,  5.6006e-04,  5.2805e-03,\n",
      "        -6.6708e-03, -3.0420e-04,  5.6556e-03, -5.0649e-03,  5.3157e-03,\n",
      "        -7.0200e-03,  1.1017e-03,  9.1937e-03,  1.5687e-02, -8.5235e-03,\n",
      "        -4.5610e-04, -2.0932e-03, -3.0682e-03,  9.3025e-03,  6.3182e-03,\n",
      "         7.3732e-03, -6.0607e-03, -8.5493e-03,  5.2426e-03, -1.7677e-02,\n",
      "        -1.8438e-03, -1.1483e-02, -2.0629e-02, -9.4936e-04, -6.0334e-03,\n",
      "         5.1381e-04,  2.6459e-03,  1.4258e-02,  9.0382e-03, -2.6761e-03,\n",
      "         6.3689e-05,  1.9364e-02,  7.7784e-03,  1.1274e-03, -2.0766e-02,\n",
      "        -1.2503e-02, -1.1147e-02, -7.8099e-03, -3.8389e-03, -1.5915e-03,\n",
      "        -5.6747e-03, -8.6282e-03,  1.7552e-04,  1.1160e-02, -1.6430e-02,\n",
      "         3.2399e-03,  1.4903e-02,  9.0778e-03, -2.7746e-02,  5.5558e-03,\n",
      "        -7.7217e-03, -8.8645e-03,  1.2973e-02, -7.3107e-03, -5.8489e-03,\n",
      "        -1.5490e-03, -1.7094e-02,  7.0134e-03, -1.2296e-02, -3.9083e-04,\n",
      "         4.4045e-03, -8.0799e-03, -4.3148e-03, -1.0194e-02,  4.8176e-03,\n",
      "        -4.7038e-03,  7.9731e-04,  1.7453e-02, -1.8569e-03,  1.1612e-02,\n",
      "        -6.4087e-03, -8.3547e-03, -1.1936e-03,  1.6696e-02,  2.2233e-03,\n",
      "        -1.2055e-03, -2.0337e-03,  1.4509e-02, -1.4651e-03,  1.4339e-02,\n",
      "         7.7450e-03, -2.3270e-03, -8.1712e-03, -8.2093e-03,  1.1724e-02,\n",
      "         1.5864e-03,  1.7247e-02,  2.4282e-04,  9.9412e-03,  2.7961e-03,\n",
      "        -2.1477e-02, -1.3934e-02,  9.4469e-03,  2.8900e-03,  4.9035e-03,\n",
      "        -4.4759e-04, -6.4416e-03, -1.0590e-02, -1.2241e-02,  1.6421e-02,\n",
      "         8.7718e-03, -1.4605e-02, -1.6292e-03, -1.7565e-02,  5.7276e-03,\n",
      "        -9.2560e-03,  1.4746e-02, -1.0187e-02,  1.2708e-02, -9.4510e-03,\n",
      "        -2.8337e-03, -1.3747e-02, -7.3946e-04, -9.1474e-03, -3.0278e-03,\n",
      "        -1.3384e-02, -1.1215e-03, -3.5580e-03, -1.0749e-02,  1.3149e-02,\n",
      "        -6.2476e-04,  1.6040e-02,  9.8849e-03,  6.7211e-03, -1.7025e-03,\n",
      "        -5.3056e-03, -5.4859e-03,  1.6702e-02,  1.3575e-02,  4.2678e-03,\n",
      "        -4.0552e-03,  7.1860e-03,  1.1300e-03,  1.9961e-03,  1.0843e-04,\n",
      "         4.0022e-03, -5.0452e-04, -8.1985e-03, -7.5422e-03, -1.7033e-02,\n",
      "         9.0706e-04, -1.4799e-02,  1.1202e-02, -1.1938e-02,  1.4012e-02,\n",
      "         1.5941e-02,  1.4085e-02, -1.6555e-02, -1.6673e-03,  1.9451e-04,\n",
      "        -9.5278e-03, -2.0111e-02,  5.5086e-03, -2.5169e-03,  4.6339e-04,\n",
      "        -1.2430e-02,  3.4462e-03, -4.8552e-03,  1.5150e-02,  3.0073e-03,\n",
      "         8.2400e-03, -5.5945e-03,  1.4301e-02, -8.6275e-03,  6.3746e-03,\n",
      "        -2.5850e-02,  1.4954e-03, -8.5342e-03,  4.6720e-03,  6.7994e-03,\n",
      "         1.5794e-02, -1.9232e-03,  6.3751e-03, -1.3060e-02,  1.1090e-02,\n",
      "         1.5993e-02,  2.3649e-02, -4.7302e-03,  6.5247e-04, -2.4151e-04,\n",
      "        -4.8639e-03, -3.7603e-03, -2.4734e-02, -1.4594e-02,  1.2413e-03,\n",
      "        -3.6925e-03,  3.3949e-03,  1.9166e-02, -5.7200e-03,  1.3845e-02,\n",
      "         6.9732e-03,  1.0192e-02,  7.2801e-03, -5.0765e-03, -1.3697e-02,\n",
      "        -1.8452e-02, -1.0004e-02,  6.9512e-03, -1.2998e-03,  1.0453e-02,\n",
      "        -4.8022e-03,  9.5852e-03,  3.7022e-03, -1.6873e-03,  9.5052e-03,\n",
      "         1.2824e-02,  9.2000e-03,  6.2039e-03, -8.7111e-03, -5.9686e-03,\n",
      "         6.8673e-03, -2.0097e-03,  3.7464e-03,  1.3780e-02,  1.0768e-02,\n",
      "         2.8114e-02, -2.0066e-02, -2.8605e-04,  4.4243e-03,  2.4939e-03,\n",
      "         1.4749e-02,  4.8135e-03,  5.6929e-03, -2.4414e-03,  3.3202e-03,\n",
      "        -1.3390e-02, -1.0807e-02, -1.9766e-02,  2.2767e-02, -1.3472e-02,\n",
      "         5.1177e-03,  2.8282e-03,  4.5411e-03,  9.4432e-03, -1.0301e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.mean.weight\n",
      "torch.Size([784, 500])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0059,  0.0062,  0.0090,  ...,  0.0021, -0.0060, -0.0094],\n",
      "        [ 0.0072, -0.0059,  0.0276,  ...,  0.0070, -0.0158,  0.0085],\n",
      "        [ 0.0066,  0.0051, -0.0019,  ...,  0.0085, -0.0078,  0.0021],\n",
      "        ...,\n",
      "        [-0.0133,  0.0106, -0.0040,  ..., -0.0090,  0.0079, -0.0085],\n",
      "        [ 0.0169, -0.0011, -0.0062,  ...,  0.0092, -0.0059,  0.0049],\n",
      "        [ 0.0012, -0.0046, -0.0094,  ...,  0.0085, -0.0065, -0.0043]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.mean.bias\n",
      "torch.Size([784])\n",
      "Parameter containing:\n",
      "tensor([ 8.0164e-04, -8.0679e-03, -5.5487e-03,  3.9743e-03,  4.0668e-03,\n",
      "         6.4585e-03, -1.6338e-02, -2.4798e-02,  1.4825e-02, -5.0586e-03,\n",
      "        -5.9736e-03, -8.1194e-03, -8.6554e-04, -1.2441e-02, -2.6146e-02,\n",
      "        -1.3487e-02, -1.0098e-03,  5.7329e-03, -1.1705e-02, -6.4012e-03,\n",
      "        -3.5960e-03, -1.9166e-02, -1.5013e-02,  1.5680e-02, -1.0263e-02,\n",
      "        -1.0693e-02,  1.2421e-02, -2.0772e-03, -1.1615e-03,  1.8946e-02,\n",
      "         1.6188e-03, -4.8602e-03, -7.0827e-04,  7.2210e-04, -4.7190e-03,\n",
      "        -1.5062e-02, -8.4427e-03,  8.4071e-03, -1.0617e-02,  6.2564e-03,\n",
      "        -1.4443e-03,  1.0360e-02, -1.8867e-02,  4.2234e-03, -4.0301e-03,\n",
      "        -1.6103e-02,  5.6932e-03, -1.5340e-02,  3.3333e-03,  6.4730e-03,\n",
      "        -1.1863e-02,  2.2009e-03,  3.9752e-03,  3.2887e-03, -1.5988e-03,\n",
      "         4.3866e-03, -1.3120e-02,  5.3978e-03, -1.4085e-02, -3.5678e-03,\n",
      "         6.8660e-04, -4.8071e-03,  8.7232e-03, -1.4070e-02,  1.8837e-03,\n",
      "         1.4452e-02,  1.2945e-03, -1.5802e-02, -1.4659e-02, -1.0513e-02,\n",
      "         1.4611e-02,  1.7070e-02, -2.0519e-03,  1.7662e-02,  5.1705e-03,\n",
      "        -5.6584e-03,  1.5285e-02,  7.1428e-03,  3.9864e-03, -2.6327e-03,\n",
      "         2.6301e-02, -1.7200e-02,  7.8361e-03,  4.8106e-03, -2.6677e-03,\n",
      "         1.0348e-02,  1.9129e-02,  6.2478e-03,  1.2646e-02,  1.8699e-02,\n",
      "        -1.9474e-03,  1.1569e-02,  1.8665e-03,  8.2159e-03,  1.4838e-03,\n",
      "         3.2915e-03, -1.0002e-02, -1.0700e-02, -1.1857e-02, -5.9707e-03,\n",
      "         9.4164e-03, -4.7201e-03, -7.6976e-03, -5.8380e-03,  9.5249e-03,\n",
      "        -1.6049e-02,  1.3006e-03,  9.6195e-04,  1.1664e-02,  1.7439e-03,\n",
      "        -5.4652e-03, -6.9550e-04, -1.0453e-02, -6.9417e-03, -4.5128e-03,\n",
      "        -4.7197e-03,  1.5022e-02,  5.6272e-03,  5.0487e-03,  1.1986e-02,\n",
      "        -1.2100e-02, -6.2184e-03,  5.2334e-03, -1.3654e-02,  1.1105e-02,\n",
      "        -9.8297e-03, -9.4438e-04,  1.9964e-03,  9.9055e-03,  9.0666e-03,\n",
      "         4.7412e-03,  6.8422e-03, -7.0712e-03,  1.8107e-02,  8.4217e-03,\n",
      "         1.1380e-03,  2.6010e-03,  4.5342e-04,  4.6151e-05,  8.6508e-03,\n",
      "         8.3021e-03,  1.5449e-02,  3.2291e-03, -7.6680e-03, -6.3016e-03,\n",
      "        -1.3661e-04, -9.1407e-03,  1.0504e-02,  7.4783e-03,  1.3609e-02,\n",
      "         6.1262e-04, -7.7542e-03, -1.0728e-02, -2.1824e-03, -4.3541e-03,\n",
      "        -8.3555e-03,  1.1030e-02, -1.5221e-02, -1.0148e-02,  7.6874e-03,\n",
      "         8.6750e-05,  1.2169e-02,  8.1170e-03, -4.2516e-03,  3.1471e-03,\n",
      "        -1.5879e-03,  1.0630e-02,  1.4413e-02, -2.7228e-03, -4.6350e-03,\n",
      "        -1.3420e-02, -8.0065e-03, -7.7705e-03,  1.7314e-02, -1.4656e-02,\n",
      "         7.6743e-03,  1.3398e-02,  1.9264e-02, -4.2555e-03, -5.0208e-03,\n",
      "         6.7041e-03,  4.0165e-03, -1.0746e-02, -2.1032e-05,  9.4292e-03,\n",
      "        -4.9686e-03,  1.0487e-03, -4.1863e-03,  4.4956e-03,  2.4074e-04,\n",
      "         1.1407e-02, -7.3457e-03,  1.7921e-03, -6.6346e-04, -5.0290e-03,\n",
      "        -9.8140e-04,  5.5810e-03,  8.5389e-03, -3.5072e-03, -1.1405e-02,\n",
      "         1.6228e-02, -2.0192e-03, -4.6060e-03, -5.6523e-03, -9.8191e-03,\n",
      "         2.2215e-02,  4.4053e-03,  6.8439e-03,  8.5856e-03,  1.1365e-02,\n",
      "         1.2106e-02, -1.1178e-02,  1.5297e-02,  4.3394e-03,  2.0292e-02,\n",
      "         1.8065e-02,  2.4479e-03, -2.8601e-03,  7.8692e-03, -1.0862e-02,\n",
      "        -1.3596e-02, -8.7925e-04,  3.2512e-03,  8.7408e-03, -8.5400e-03,\n",
      "         2.3714e-03,  2.6941e-03, -2.3978e-02,  5.0025e-03,  5.5680e-03,\n",
      "        -3.3327e-03,  4.1671e-03,  2.6987e-03, -1.2628e-02,  6.6095e-03,\n",
      "         2.4952e-02, -5.3721e-03, -5.9746e-03,  1.0258e-02,  3.1452e-03,\n",
      "        -1.0834e-02,  1.7839e-04,  1.0715e-02,  1.1632e-02, -1.6380e-02,\n",
      "        -1.7547e-02, -1.1441e-02,  8.7617e-03,  9.6876e-03,  2.3853e-04,\n",
      "         4.3653e-03,  1.8621e-02,  4.1629e-03, -1.6541e-02,  6.1923e-03,\n",
      "         6.3846e-03, -4.5239e-03,  1.9775e-02,  6.9925e-03, -1.5298e-02,\n",
      "        -8.9654e-03,  4.0461e-03,  8.8057e-03,  8.4491e-04,  8.9102e-04,\n",
      "        -7.3048e-03,  5.3830e-03,  5.9470e-04,  3.7144e-03, -8.6680e-03,\n",
      "        -3.1970e-03,  9.2093e-03,  7.0924e-03,  1.4614e-02, -6.6442e-03,\n",
      "         3.5042e-03, -7.1201e-03,  8.7872e-03, -9.1274e-03, -4.3887e-03,\n",
      "         7.4688e-03,  2.6328e-03,  4.9886e-03, -6.8166e-03,  1.0168e-02,\n",
      "        -9.1385e-03, -2.1396e-02, -4.8120e-03,  2.1577e-02,  1.1465e-02,\n",
      "         3.7611e-03, -1.2998e-02, -9.2917e-03,  1.5346e-03,  9.3141e-03,\n",
      "        -2.9904e-03,  9.1452e-04, -2.3481e-03, -1.4302e-02, -2.4191e-03,\n",
      "         1.5978e-02, -6.5716e-03, -1.7440e-02, -6.5823e-03,  6.1638e-03,\n",
      "        -8.6352e-03,  2.7639e-02,  9.1159e-03,  4.5838e-03, -9.1125e-04,\n",
      "         2.8291e-03,  1.9579e-03,  5.9528e-03, -3.0570e-03,  4.9617e-03,\n",
      "         1.2280e-03, -5.6659e-03,  1.7298e-03,  1.5327e-03,  1.4415e-02,\n",
      "         7.4625e-04, -2.6261e-04,  2.5484e-03, -1.1627e-02, -2.9571e-03,\n",
      "         1.1549e-02,  1.1066e-02,  1.4827e-02,  1.1471e-02,  7.6880e-03,\n",
      "         6.6853e-03,  1.8637e-02,  1.5745e-02,  1.0842e-02,  1.5030e-02,\n",
      "        -9.0207e-03, -1.1018e-03,  4.6456e-03, -1.2586e-03,  1.5020e-04,\n",
      "        -2.6452e-03,  6.9674e-04, -1.5794e-02,  6.8299e-03, -2.1705e-03,\n",
      "        -1.5525e-02, -1.9619e-03, -7.7465e-04,  1.8997e-03,  1.3784e-03,\n",
      "        -2.3077e-02,  1.7902e-02,  3.9755e-03,  8.7902e-03,  1.0475e-02,\n",
      "         7.5435e-03, -3.1275e-03,  1.5138e-02, -1.6131e-02,  8.9867e-03,\n",
      "         9.9534e-03, -1.0915e-02, -5.1673e-03,  8.1021e-03, -6.3599e-03,\n",
      "         9.4070e-03,  6.4034e-03,  7.7173e-04, -2.0634e-03, -2.3121e-02,\n",
      "         5.5913e-03, -3.5903e-05, -1.0402e-02, -3.9071e-03,  1.7393e-02,\n",
      "         7.0602e-03,  1.2436e-02,  4.0100e-03,  1.1317e-02, -2.0279e-02,\n",
      "         4.8309e-03,  4.8482e-03,  1.8624e-02, -9.1647e-03,  2.7024e-03,\n",
      "         1.0653e-02,  2.7582e-03, -2.3408e-02,  1.5191e-02,  2.6556e-03,\n",
      "        -5.7485e-03, -6.5376e-03, -6.7294e-03, -1.7755e-02, -3.3326e-03,\n",
      "        -1.0592e-02,  3.4601e-03, -1.0675e-02, -8.8047e-03, -8.2255e-04,\n",
      "        -1.5302e-03,  5.8463e-03, -1.2596e-02,  1.6307e-02,  2.4193e-03,\n",
      "         5.0083e-03, -1.8295e-03, -1.9080e-03, -1.5745e-02,  5.9589e-03,\n",
      "         3.6291e-03, -2.7425e-03, -9.6162e-03, -5.3567e-03,  1.2634e-02,\n",
      "         9.0999e-03,  8.4591e-03, -1.3393e-02,  1.2207e-02,  7.1218e-03,\n",
      "        -6.0573e-03,  1.1401e-02, -1.1477e-02,  1.5711e-02,  1.5490e-03,\n",
      "         2.0699e-03, -1.0846e-02,  4.2072e-03, -5.8346e-03,  1.7268e-02,\n",
      "         3.1135e-03, -2.1736e-03, -1.0551e-02, -3.9215e-03, -1.7681e-03,\n",
      "        -2.4694e-03,  4.9090e-03, -1.0984e-03, -1.4398e-02,  5.3815e-03,\n",
      "        -4.0269e-03,  9.8604e-03,  4.3613e-03,  1.8168e-02,  1.5899e-02,\n",
      "        -1.3747e-02,  1.3224e-02, -1.2263e-02,  1.9526e-02, -2.9572e-03,\n",
      "        -6.0809e-03, -3.8694e-03, -1.5907e-04,  1.4159e-03,  8.8411e-03,\n",
      "        -3.6376e-03, -8.0532e-03, -6.2766e-03,  1.8418e-03,  1.5151e-03,\n",
      "        -1.6467e-02, -7.9545e-04,  1.1556e-02,  2.0644e-02,  9.8216e-03,\n",
      "        -1.2003e-02, -2.2096e-03, -1.8827e-03, -9.9586e-03, -1.3383e-03,\n",
      "        -1.6807e-02, -2.2481e-03,  2.5321e-03,  1.1779e-02, -1.0768e-02,\n",
      "        -1.1857e-02,  2.8293e-02, -2.4001e-02,  3.5075e-04,  9.2921e-03,\n",
      "        -9.3302e-03, -2.3977e-02, -1.0298e-02, -2.3175e-03, -2.6435e-04,\n",
      "         7.1320e-03, -1.6174e-02, -1.5755e-02,  1.2884e-02,  1.4292e-03,\n",
      "         4.2296e-03, -6.2099e-03,  1.8721e-03, -2.3089e-03, -7.4294e-04,\n",
      "         2.1320e-03,  6.2923e-03,  3.3503e-03, -1.0957e-02,  2.0016e-02,\n",
      "        -6.9924e-05, -3.5399e-03,  9.9748e-03,  8.2496e-03, -6.9662e-03,\n",
      "         2.5814e-03,  3.8489e-03, -1.3446e-02, -1.0593e-02, -2.9893e-03,\n",
      "         7.8365e-03,  6.7744e-03, -4.4800e-03, -2.2219e-02, -6.4979e-03,\n",
      "        -6.1449e-03,  2.2914e-03,  9.3886e-03, -1.1886e-02, -1.7175e-03,\n",
      "        -8.1938e-03, -3.7481e-03, -1.4862e-02, -8.1711e-03,  9.1069e-03,\n",
      "        -2.1022e-02,  1.1526e-02, -2.1497e-02,  3.8773e-03,  6.5909e-03,\n",
      "        -5.8840e-03, -4.9746e-03,  1.9482e-03, -6.6444e-03,  5.8595e-05,\n",
      "         6.6937e-03,  7.3511e-03,  3.1582e-03, -9.7502e-03,  1.8498e-02,\n",
      "         8.9996e-03,  7.2242e-03,  1.3005e-02,  1.3608e-02,  4.6805e-03,\n",
      "         5.0569e-03,  8.1058e-03,  7.9996e-03,  1.0408e-04, -1.1705e-03,\n",
      "        -1.7487e-02, -5.2058e-03, -1.1033e-02, -1.5270e-02,  1.1439e-03,\n",
      "        -1.9330e-02,  4.1294e-03,  8.6376e-03, -5.1292e-03,  3.0413e-03,\n",
      "        -1.8879e-02,  1.0139e-03, -8.6121e-03, -5.3649e-03, -6.6467e-03,\n",
      "         3.1516e-03, -2.8249e-03,  1.7439e-02,  1.8348e-02, -7.7521e-03,\n",
      "        -6.2118e-03, -5.2910e-04, -1.2095e-03,  8.3523e-04,  4.4759e-03,\n",
      "         6.6968e-03, -4.8230e-03,  1.6585e-02, -4.7052e-03,  1.3140e-02,\n",
      "         3.0588e-03, -5.0158e-03, -1.7395e-02,  1.8279e-02, -1.0386e-02,\n",
      "        -2.9045e-03, -1.4672e-02, -7.7852e-03,  2.3316e-03,  9.8351e-04,\n",
      "        -9.1619e-03, -1.5327e-03, -9.0899e-03,  1.6127e-02, -5.9038e-03,\n",
      "        -1.3580e-02,  9.9037e-04,  5.0855e-03, -9.1105e-03, -8.9144e-03,\n",
      "        -5.7506e-03,  3.9831e-03,  6.0361e-03, -4.9054e-03, -7.7767e-03,\n",
      "        -1.2046e-02, -7.4338e-03, -3.0594e-03,  5.8796e-03,  1.1126e-02,\n",
      "        -3.4430e-03, -1.3418e-02, -9.6539e-03, -4.1389e-03, -1.2804e-02,\n",
      "         1.6386e-02,  6.9784e-03,  8.1411e-04,  1.7759e-02, -8.4252e-03,\n",
      "        -3.8273e-03,  1.0728e-02, -9.0234e-03, -9.2837e-03,  7.2813e-03,\n",
      "        -1.3396e-03,  5.3709e-03,  6.7336e-03,  7.3873e-03, -6.6470e-03,\n",
      "         8.8590e-03,  5.8714e-03,  1.3118e-02,  7.3356e-03,  1.5192e-03,\n",
      "        -6.8347e-03, -2.5662e-02, -8.7535e-03, -8.7879e-03, -1.5768e-03,\n",
      "        -4.4049e-03, -5.0024e-03, -3.5112e-03,  9.7926e-03, -7.4923e-03,\n",
      "         1.3655e-02, -1.4440e-02,  1.6945e-02, -6.4263e-03,  1.5776e-02,\n",
      "        -2.1761e-03,  3.4056e-03, -2.1604e-03, -1.8212e-02,  1.2175e-03,\n",
      "        -4.3397e-04, -2.5121e-03,  2.0858e-02,  1.1326e-02, -5.1665e-03,\n",
      "        -7.9895e-03,  1.2652e-02,  1.1955e-02,  1.8307e-02,  4.8539e-03,\n",
      "        -4.0127e-04,  4.5277e-03,  2.2954e-03, -8.1113e-03,  2.5127e-03,\n",
      "         9.0338e-04,  5.8819e-03, -4.6016e-03,  4.3490e-03, -9.7560e-04,\n",
      "        -9.8582e-03, -3.7014e-03, -4.4642e-03,  7.3405e-03,  4.7203e-03,\n",
      "        -1.3650e-02, -1.4262e-02,  5.1325e-03, -4.0181e-03, -1.0940e-02,\n",
      "         2.0404e-03,  9.7063e-03,  1.0402e-02, -2.9196e-03,  1.5062e-02,\n",
      "        -3.3669e-03,  5.3837e-03,  1.4549e-02,  1.0940e-02,  7.9023e-03,\n",
      "         1.2655e-02,  1.0804e-02,  6.9365e-03, -8.9747e-03,  3.3137e-03,\n",
      "         7.0590e-03, -1.9505e-02, -3.9119e-03,  5.9731e-03, -1.4630e-02,\n",
      "         2.3038e-03,  6.2607e-03, -9.2779e-04,  1.8365e-02,  1.4037e-02,\n",
      "        -2.2230e-02,  8.6137e-03,  1.0558e-03, -1.0774e-02, -7.2252e-04,\n",
      "         5.5782e-03, -1.1302e-02,  1.4094e-02,  1.1186e-03,  4.8749e-03,\n",
      "        -3.2765e-03,  1.1889e-02, -1.6104e-03, -7.6059e-03,  9.2360e-03,\n",
      "         3.2115e-03,  1.6519e-02,  1.1188e-02,  4.2039e-03,  1.1478e-02,\n",
      "        -1.1202e-03, -5.0908e-03,  9.9735e-03,  1.5000e-02, -1.2984e-02,\n",
      "        -6.8348e-03,  1.3045e-03, -2.4853e-03, -8.1254e-03,  6.6635e-03,\n",
      "         4.5657e-03,  6.2587e-03,  1.3011e-02, -1.5609e-03, -1.2127e-02,\n",
      "        -5.6434e-03,  1.2203e-02,  6.8557e-03,  5.1323e-03, -1.3297e-02,\n",
      "        -1.3026e-02, -9.3067e-03,  9.9603e-03,  2.6218e-03, -5.6758e-03,\n",
      "         3.4356e-03,  2.7076e-03,  4.2506e-03, -1.1976e-02, -1.1748e-02,\n",
      "         2.3325e-03,  5.6809e-03,  1.8960e-03, -2.7791e-03,  1.9393e-03,\n",
      "         2.1983e-02, -6.9073e-03, -3.3898e-03,  1.1423e-02, -9.4984e-03,\n",
      "        -2.6917e-03, -2.1037e-02,  4.0085e-03, -1.4097e-03, -3.5300e-03,\n",
      "        -4.9323e-03,  1.4235e-03, -3.3563e-03, -9.4254e-03, -1.9866e-03,\n",
      "         4.9786e-03, -2.0859e-03,  4.9243e-03, -1.7674e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "decoder.log_var.weight\n",
      "torch.Size([784, 500])\n",
      "Parameter containing:\n",
      "tensor([[-0.0077, -0.0077, -0.0064,  ..., -0.0001,  0.0045, -0.0094],\n",
      "        [ 0.0040,  0.0108,  0.0131,  ..., -0.0085, -0.0073,  0.0173],\n",
      "        [-0.0070, -0.0007,  0.0025,  ..., -0.0008, -0.0066,  0.0154],\n",
      "        ...,\n",
      "        [-0.0059,  0.0111, -0.0152,  ...,  0.0142,  0.0003, -0.0033],\n",
      "        [-0.0019, -0.0121, -0.0080,  ...,  0.0208,  0.0151,  0.0025],\n",
      "        [-0.0102, -0.0154, -0.0028,  ..., -0.0069,  0.0019, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "decoder.log_var.bias\n",
      "torch.Size([784])\n",
      "Parameter containing:\n",
      "tensor([ 6.7914e-03, -3.6265e-04,  1.9021e-02, -3.7838e-04, -9.9135e-03,\n",
      "        -1.4779e-02,  1.1580e-02, -1.3651e-02, -3.2192e-03, -5.7603e-03,\n",
      "         8.2469e-03,  3.2138e-03,  5.1025e-03, -3.7840e-03, -7.7381e-04,\n",
      "         2.7180e-02, -5.6308e-03,  1.3601e-02,  1.1438e-02,  5.9952e-03,\n",
      "         5.5215e-04, -1.2172e-02,  1.2992e-03, -1.6598e-02,  2.1581e-02,\n",
      "        -8.7773e-03,  1.4107e-02, -3.7228e-03,  3.2458e-03,  1.3610e-02,\n",
      "         5.1528e-04,  1.6342e-02,  5.6695e-03,  2.0147e-02, -1.3497e-02,\n",
      "        -1.7720e-04,  5.1940e-03,  2.8954e-03,  5.1431e-03, -8.5331e-03,\n",
      "        -5.4434e-04, -4.3912e-03, -3.8420e-04,  1.3585e-03,  1.8945e-02,\n",
      "        -5.8565e-03, -1.1985e-02, -9.6333e-06,  6.2384e-03,  4.8776e-03,\n",
      "        -1.2064e-02,  2.2458e-03, -9.8992e-03, -3.7191e-03,  4.3967e-03,\n",
      "        -7.6585e-03, -1.6949e-02, -8.0999e-03,  9.7153e-03,  4.8979e-03,\n",
      "         6.0342e-03, -7.6305e-03, -6.5998e-03, -1.4950e-02,  1.6044e-02,\n",
      "         7.8230e-03,  3.9788e-03,  1.1311e-02, -6.3974e-03, -9.0351e-04,\n",
      "         5.9803e-03,  4.9090e-03, -3.1822e-03,  8.6911e-03, -1.4016e-03,\n",
      "         1.5424e-03, -9.6531e-03,  1.5182e-03, -9.9684e-03,  1.4444e-03,\n",
      "         2.6030e-03, -8.9071e-03,  1.4314e-02, -7.3071e-03,  1.1039e-02,\n",
      "         8.6149e-03, -6.0911e-03,  7.2899e-03,  1.6369e-02, -4.1273e-03,\n",
      "         5.9406e-03,  7.1370e-03,  1.4378e-03,  1.6651e-02, -2.1730e-02,\n",
      "         1.0256e-02,  9.1122e-03, -7.6337e-03, -1.0923e-02,  1.6158e-03,\n",
      "         6.1483e-03,  4.4941e-03,  1.3846e-02, -2.6738e-03,  1.0193e-03,\n",
      "        -3.5739e-03,  8.3894e-03,  9.4660e-03,  4.8484e-03,  4.8908e-04,\n",
      "         8.1986e-04, -7.2183e-03, -3.6459e-03,  1.1692e-02,  8.0978e-03,\n",
      "        -1.3221e-02,  5.4527e-03, -9.0656e-04, -1.1515e-03,  4.0680e-03,\n",
      "         5.8200e-03,  3.1656e-03, -4.4892e-03, -9.6862e-03,  2.3091e-03,\n",
      "        -6.5876e-03, -2.0495e-03, -1.9920e-03,  1.3181e-02,  6.1370e-03,\n",
      "         1.1174e-03,  1.5113e-03,  8.6400e-03, -1.3776e-03,  4.1735e-04,\n",
      "         1.3299e-02, -3.7818e-03, -1.0676e-02,  5.2323e-04,  1.2574e-03,\n",
      "        -1.1025e-02,  5.0294e-03,  2.9268e-04, -2.4688e-03, -8.7761e-03,\n",
      "         5.0125e-03, -6.8886e-03, -1.8994e-03, -2.0614e-02,  2.4134e-06,\n",
      "         7.8286e-03,  7.6707e-03, -4.2326e-03, -1.6870e-03,  1.2029e-02,\n",
      "        -1.2059e-02,  5.9677e-03,  1.5533e-02,  1.6242e-02, -5.5964e-03,\n",
      "        -1.4825e-02,  5.0696e-03,  1.2005e-02,  7.1701e-03, -9.4662e-03,\n",
      "        -1.0820e-03, -1.6543e-02,  2.1193e-03, -7.6393e-03, -8.8279e-03,\n",
      "         7.7312e-03,  2.9135e-03,  4.4409e-03, -9.3166e-03,  5.5060e-03,\n",
      "         9.2190e-03, -6.5050e-03,  6.2955e-03, -1.8240e-03,  1.5132e-02,\n",
      "         8.9929e-03,  1.4623e-02,  1.4396e-02, -9.4269e-03, -4.4039e-03,\n",
      "         1.1069e-02,  9.5795e-03,  3.7792e-03,  1.3379e-02, -9.4997e-03,\n",
      "        -1.0389e-02,  4.5258e-04,  4.9574e-03,  1.8704e-02, -8.5545e-03,\n",
      "         3.3322e-03, -1.2660e-02, -1.0681e-02,  1.0288e-02,  1.0430e-02,\n",
      "         1.5505e-02,  6.5318e-03,  5.1708e-03, -1.6693e-02, -1.7871e-02,\n",
      "        -4.3580e-03, -1.3276e-02, -5.5373e-03, -8.2334e-03,  1.0893e-02,\n",
      "         9.7878e-03, -1.0646e-03, -1.8739e-02, -1.3560e-03,  3.4630e-03,\n",
      "        -1.7136e-02, -1.5719e-03, -2.2261e-03,  7.5037e-04,  6.5556e-03,\n",
      "        -6.7201e-03,  7.0708e-03,  2.4676e-03, -1.0655e-02, -8.1667e-03,\n",
      "        -1.7980e-04, -2.6457e-03,  3.4636e-03, -4.4413e-03,  6.8705e-04,\n",
      "         9.5871e-03,  1.0424e-02, -2.9738e-03, -4.7195e-03, -1.9689e-03,\n",
      "        -1.6805e-02,  6.4254e-03, -4.3455e-03,  4.1259e-05,  1.4704e-02,\n",
      "        -9.6341e-03,  1.1990e-02, -3.5140e-03,  9.7659e-03, -2.6501e-03,\n",
      "        -9.1351e-03,  2.8250e-03,  6.0299e-03, -1.2493e-02,  8.9908e-03,\n",
      "         1.0768e-02, -2.0625e-03, -2.0569e-02, -5.1021e-03, -8.9886e-03,\n",
      "        -6.7240e-03,  5.8061e-04,  1.8490e-03,  6.2443e-04, -1.7310e-02,\n",
      "         6.0045e-03,  8.5323e-04,  1.8993e-02,  9.4725e-03,  8.4670e-03,\n",
      "         1.4684e-02,  2.0030e-04,  1.4457e-02, -7.0472e-03,  4.6576e-03,\n",
      "        -3.3636e-03, -7.4509e-03,  7.4919e-03, -2.6991e-03, -2.3100e-02,\n",
      "         1.3138e-02,  7.7488e-03, -4.9482e-03,  2.8093e-03, -2.4448e-03,\n",
      "        -1.4275e-02, -1.0040e-02,  2.3146e-03,  1.1691e-02,  6.8117e-04,\n",
      "         1.5395e-03,  2.4877e-03, -7.2966e-03,  1.2186e-02,  5.7558e-03,\n",
      "        -9.7194e-03,  1.5601e-03, -1.3393e-02, -1.0831e-04,  5.2293e-03,\n",
      "         2.9183e-04, -6.1344e-03, -1.8927e-03,  5.5247e-03,  4.7931e-04,\n",
      "         3.2196e-03, -8.4375e-03, -5.0549e-03, -8.1706e-03,  1.0580e-02,\n",
      "         2.7406e-03, -9.2702e-03, -2.4038e-03, -8.4105e-03, -5.0603e-04,\n",
      "        -8.5848e-03, -6.8768e-03,  6.4973e-03,  6.2543e-03,  1.4078e-03,\n",
      "         1.4088e-02,  6.2496e-03,  5.8030e-03,  9.4223e-03,  6.7946e-03,\n",
      "        -9.2062e-03,  1.9287e-02,  3.1195e-03,  7.5751e-04,  1.2836e-02,\n",
      "         9.0178e-03,  5.4297e-03, -3.3609e-03,  1.2093e-02,  8.6979e-04,\n",
      "         7.4018e-03, -6.4955e-04, -2.6456e-02,  9.7107e-03,  1.9099e-03,\n",
      "        -3.5228e-03, -1.8298e-03,  1.3301e-02,  8.2237e-03,  6.9062e-04,\n",
      "         1.3603e-02,  1.9450e-03,  1.3348e-02, -7.0814e-03,  5.8592e-03,\n",
      "        -6.7193e-04, -7.5787e-05,  6.2656e-03, -4.0758e-03, -9.2920e-03,\n",
      "         6.1625e-03,  1.9563e-02, -4.4558e-03, -4.3586e-03,  5.6937e-04,\n",
      "         4.0097e-03,  8.4422e-05, -2.1753e-02,  9.1422e-03, -7.4453e-03,\n",
      "        -8.2981e-03, -1.1476e-02, -7.6895e-03,  1.3457e-02, -7.8266e-03,\n",
      "        -7.8164e-03, -1.3882e-02, -1.6331e-02, -6.9120e-03,  1.1982e-02,\n",
      "        -1.1401e-02, -7.3661e-04, -3.5837e-03,  1.2409e-02,  4.3538e-03,\n",
      "         4.7869e-03, -7.9303e-03, -2.7152e-03, -3.2331e-03,  3.7430e-03,\n",
      "        -2.1161e-02, -2.1599e-03,  4.0534e-03,  1.2724e-03, -5.0980e-03,\n",
      "         3.4630e-02, -8.9198e-03, -8.6947e-03, -1.2481e-02,  1.9635e-03,\n",
      "         1.9038e-02,  6.7989e-03,  2.5255e-05, -7.8306e-03, -1.1497e-02,\n",
      "         1.1071e-03,  1.0844e-02,  1.0356e-03, -3.5576e-03,  1.0459e-02,\n",
      "         1.1931e-03, -5.9685e-03, -2.3712e-03,  1.4210e-02,  1.8660e-02,\n",
      "        -2.0099e-04,  9.5803e-03, -3.2488e-03, -9.7825e-03, -4.7953e-04,\n",
      "        -9.6137e-03,  1.2311e-02, -9.9839e-04,  5.4142e-03, -5.4337e-03,\n",
      "        -5.3923e-03,  3.1489e-03, -3.4641e-03, -2.9871e-04,  1.6217e-03,\n",
      "        -6.4994e-03,  6.8195e-03, -2.7904e-03,  7.9280e-04,  8.4520e-03,\n",
      "         7.7760e-04,  1.0007e-02,  3.0672e-03,  1.0793e-03,  7.6872e-03,\n",
      "        -1.4408e-02, -4.5488e-03,  6.1897e-03, -9.6915e-03, -7.9689e-03,\n",
      "        -9.1301e-03,  1.2322e-02, -5.3520e-04, -5.9088e-03,  1.5591e-02,\n",
      "        -8.2664e-04,  5.8901e-03, -6.1980e-03,  8.6437e-03,  1.4162e-02,\n",
      "         1.9945e-02,  1.4636e-04, -8.2938e-03, -1.9130e-02,  3.5131e-03,\n",
      "         1.4319e-02,  4.6092e-03, -5.3199e-03,  1.0710e-02,  9.4985e-03,\n",
      "        -5.5397e-03,  1.3388e-02, -2.3948e-02,  1.3743e-02,  1.3640e-02,\n",
      "        -2.5858e-03,  1.1203e-02, -1.2319e-02, -1.0247e-02, -2.8517e-03,\n",
      "         6.1259e-03,  1.5765e-02, -1.5572e-02,  7.1803e-03, -9.4299e-03,\n",
      "         4.8789e-03,  4.3790e-03, -2.8775e-03, -3.4191e-03, -1.7359e-02,\n",
      "         5.7624e-03,  2.6460e-02,  6.5753e-03,  4.5096e-03,  1.4235e-03,\n",
      "        -6.0123e-03, -8.5980e-03, -1.9808e-03,  1.2215e-02, -2.4377e-02,\n",
      "         2.2227e-02, -1.5121e-03,  1.3505e-02,  2.0705e-02, -6.0915e-03,\n",
      "        -1.8326e-03, -8.6986e-04,  5.3322e-05, -7.2558e-03,  1.4811e-02,\n",
      "        -1.7923e-03,  7.9535e-03,  1.5232e-03,  1.3729e-02, -1.0529e-02,\n",
      "        -7.0372e-04,  9.2454e-03, -1.6655e-02, -1.2516e-02, -1.8145e-03,\n",
      "        -6.3341e-03,  8.5151e-03,  2.8556e-03,  6.7083e-03,  1.2805e-02,\n",
      "        -1.1980e-02,  6.9756e-03, -1.1055e-03, -1.0345e-02, -1.3102e-02,\n",
      "         3.4606e-02, -7.3904e-04,  6.7126e-03, -7.8498e-03,  1.5985e-02,\n",
      "        -5.7720e-03,  5.7282e-04, -2.0132e-03, -2.1199e-03, -4.3737e-03,\n",
      "         1.2092e-02,  2.1380e-03,  6.9262e-03, -1.1719e-02, -6.9451e-03,\n",
      "         2.7388e-02,  4.9692e-03,  1.1236e-02, -6.9460e-03,  4.4804e-04,\n",
      "         2.0411e-03, -7.1212e-03,  1.1663e-02, -4.5795e-03,  6.7048e-03,\n",
      "         1.3509e-02,  1.3521e-03, -1.6648e-03, -1.7134e-02, -9.1842e-03,\n",
      "         4.8968e-03,  1.3623e-02, -5.0265e-03, -1.0351e-02,  4.7449e-03,\n",
      "         6.9487e-03,  1.6889e-02,  4.1497e-03, -4.2350e-03, -4.0782e-03,\n",
      "        -9.2068e-04, -4.9168e-03,  1.3989e-03,  8.8744e-03, -8.5171e-03,\n",
      "         5.3631e-03,  1.3159e-02, -1.4589e-02,  1.9930e-03,  5.1397e-03,\n",
      "         1.8018e-03, -2.9653e-03, -5.4842e-04, -8.7130e-03, -1.1345e-02,\n",
      "         1.0331e-03, -2.2926e-03, -9.3145e-03,  1.5356e-02,  4.4828e-03,\n",
      "         1.5376e-03,  1.7309e-03, -1.0199e-02,  8.8024e-03,  1.2876e-02,\n",
      "         7.1561e-03, -4.7523e-03, -8.4090e-03,  1.3082e-02,  1.5005e-02,\n",
      "        -1.2398e-02, -7.9009e-03,  1.7082e-02, -7.6033e-03, -1.3401e-02,\n",
      "        -3.0079e-03,  9.8493e-03,  1.1684e-02, -4.6897e-03, -6.1024e-03,\n",
      "         2.3755e-03, -9.5217e-04, -4.8922e-03,  1.0117e-02,  8.4498e-03,\n",
      "         3.1619e-03,  2.7948e-03,  1.2719e-02, -8.5792e-03, -7.9615e-03,\n",
      "         5.7244e-03, -1.2689e-02, -6.6707e-03,  1.0534e-02, -6.2184e-06,\n",
      "        -1.1917e-02, -1.1341e-02, -2.0013e-02, -2.2832e-02,  2.3589e-02,\n",
      "         5.9849e-03,  2.1803e-03, -1.9731e-02,  2.2887e-03,  6.3912e-04,\n",
      "        -1.6574e-02,  2.8436e-03,  2.8769e-03,  8.8873e-03,  2.6604e-02,\n",
      "        -8.4109e-03, -3.7594e-03, -1.1711e-02, -7.2267e-03, -2.0456e-02,\n",
      "        -5.9668e-03, -4.9336e-03,  2.2063e-02, -9.2846e-03, -4.1424e-04,\n",
      "        -1.0247e-03, -2.7502e-03, -3.3959e-03,  1.1191e-02,  7.4149e-03,\n",
      "        -4.8531e-03,  9.8612e-03,  5.1328e-03, -6.7061e-03,  1.5382e-02,\n",
      "        -7.6852e-03,  1.3552e-03, -7.0251e-03, -9.8283e-03, -8.4386e-03,\n",
      "         5.6721e-04,  7.6593e-03,  5.9510e-03,  8.2445e-03,  9.9372e-04,\n",
      "        -9.5286e-03, -1.1747e-02, -1.2840e-02,  3.9167e-03,  1.5353e-02,\n",
      "         1.4781e-03,  1.1572e-02,  1.9281e-02, -6.7915e-03,  9.6027e-03,\n",
      "        -1.1911e-02, -6.2306e-03,  6.5420e-04, -1.9144e-02,  5.6816e-03,\n",
      "        -5.0234e-03,  6.2368e-03,  1.5476e-02, -5.1586e-03, -1.2631e-02,\n",
      "         5.8465e-03,  9.1568e-03,  4.2683e-03, -1.1419e-02,  7.6154e-03,\n",
      "        -3.3988e-03,  1.1212e-02, -2.6939e-02, -3.9576e-03, -1.3165e-02,\n",
      "        -1.4450e-02, -1.8185e-02,  6.9393e-03, -2.6220e-03,  8.6913e-03,\n",
      "        -3.1204e-03, -2.0593e-02, -1.2600e-02,  3.9166e-03,  1.1948e-02,\n",
      "        -9.2220e-03, -1.2790e-02, -1.9376e-03,  8.0576e-03, -6.3564e-03,\n",
      "         1.1534e-03,  2.4158e-03, -7.5169e-03, -1.1942e-02, -2.5611e-03,\n",
      "        -8.2827e-03, -7.7113e-03, -7.4989e-03, -1.5227e-02, -1.5641e-02,\n",
      "        -1.7664e-02,  5.9152e-04, -1.9502e-03,  9.5909e-03, -5.7509e-03,\n",
      "        -7.6033e-03,  1.6889e-02,  9.9492e-03,  1.1291e-02,  1.2778e-02,\n",
      "         1.3874e-02, -1.3234e-03,  2.7706e-03, -9.5992e-03,  1.7040e-03,\n",
      "         7.6147e-03,  1.4780e-02, -1.1306e-03, -9.1470e-03, -1.7110e-03,\n",
      "         1.0605e-02,  3.9010e-03,  4.7362e-03,  1.1727e-02, -3.6312e-03,\n",
      "         2.0447e-02,  2.1464e-02,  1.1044e-02,  2.0361e-03, -5.8522e-03,\n",
      "         1.2272e-04, -3.0221e-04,  2.0444e-02, -1.0576e-02, -5.0939e-03,\n",
      "         1.2855e-02,  2.2645e-03, -5.4321e-03,  1.4589e-02,  1.1019e-02,\n",
      "         1.6028e-04,  2.0709e-03,  5.4317e-03,  1.9612e-03,  6.3031e-03,\n",
      "         1.4863e-02,  1.4285e-02,  1.0714e-02, -1.2782e-02, -5.7713e-03,\n",
      "         1.8772e-03,  5.5251e-03,  1.9660e-02,  2.9794e-03,  7.2692e-03,\n",
      "        -4.8377e-03, -8.2904e-04,  7.1466e-03,  1.4180e-03, -8.7423e-03,\n",
      "         5.8544e-03,  8.5265e-03, -9.9715e-03,  9.5544e-03,  2.4032e-02,\n",
      "         4.9477e-03, -4.5984e-03,  1.6479e-02, -2.3046e-03,  3.8371e-03,\n",
      "         2.0196e-02, -7.2843e-03, -1.1843e-02, -4.7878e-03], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.normal_(m.weight, 0.0, 0.01)\n",
    "        init.normal_(m.bias, 0.0, 0.01)\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bnn] *",
   "language": "python",
   "name": "conda-env-bnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
